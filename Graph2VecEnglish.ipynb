{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grapheme Embedding Using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import tqdm\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint,shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"data/english.txt\",encoding='latin-1').read()\n",
    "text = (re.sub(r'[^\\w\\s]','',text.lower())).replace(' ','')\n",
    "\n",
    "tokens=[]\n",
    "vocab = {'$','£'}\n",
    "for term in text.split('\\n'):\n",
    "    #print(set(term))\n",
    "    vocab.update(term)\n",
    "    tokens.append(list(\"$$\"+term+\"£\"))\n",
    "shuffle(tokens)\n",
    "vocab = list(vocab)\n",
    "v=len(vocab)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph2int(graph):\n",
    "    return vocab.index(graph)\n",
    "    \n",
    "def int2graph(index):\n",
    "    return vocab[index]\n",
    "\n",
    "def int2vec(integer):\n",
    "    vec=np.zeros(len(vocab))\n",
    "    vec[integer]=1\n",
    "    return vec\n",
    "\n",
    "def graph2vec(graph):\n",
    "    return (int2vec(graph2int(graph)))\n",
    "\n",
    "def vec2graph(vec):\n",
    "    return (int2graph(np.argmax(vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$', '$', 'f', 'u', 's', 't', 'y', '£']\n"
     ]
    }
   ],
   "source": [
    "windowSize = 2\n",
    "tokens = [tokenSet for tokenSet in tokens if len(tokenSet)>=6]\n",
    "print(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 5303/15000 [01:57<03:34, 45.22it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b50e2325faac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mgraph_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwc\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mdata_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph_i\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mx_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_sample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mlabel_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph2int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph_i\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0my_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_sample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_data = torch.empty(size=(1,2,v))\n",
    "y_data = torch.empty(size=(1,1))\n",
    "\n",
    "freqs= torch.zeros(size=(1,v))\n",
    "\n",
    "i=0\n",
    "N=len(tokens)\n",
    "for word in tqdm.tqdm(tokens[:15000]):\n",
    "    wc = len(word)\n",
    "    for graph_i in range(wc-2): \n",
    "        data_sample = torch.tensor([graph2vec(word[graph_i]), graph2vec(word[graph_i+1])]).unsqueeze(0).float()\n",
    "        x_data = torch.cat([x_data,data_sample])\n",
    "        label_sample = torch.tensor([graph2int(word[graph_i+2])]).unsqueeze(0).float()\n",
    "        y_data = torch.cat([y_data,label_sample])\n",
    "        freqs += graph2vec(word[graph_i+2])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "x_data=x_data[1:,:,:]\n",
    "y_data=y_data[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([49898, 2, 28])\n",
      "torch.Size([49898, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_data.shape)\n",
    "print(y_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,grapheme_shape,hidden_units,embedding_units):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.word_shape = grapheme_shape\n",
    "        self.hidden_units = hidden_units\n",
    "        self.embedding_units = embedding_units\n",
    "        \n",
    "        self.weights_1 = nn.Parameter(torch.empty(size=(hidden_units, grapheme_shape), requires_grad=True))\n",
    "        nn.init.normal_(self.weights_1)\n",
    "        \n",
    "        self.weights_2 = nn.Parameter(torch.empty(size=(embedding_units, hidden_units), requires_grad=True))\n",
    "        nn.init.normal_(self.weights_2)\n",
    "        \n",
    "        self.bias1 = nn.Parameter(torch.zeros(hidden_units), requires_grad=True)\n",
    "        self.bias2 = nn.Parameter(torch.zeros(embedding_units), requires_grad=True)\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        out = F.linear(inputs, self.weights_1, self.bias1)\n",
    "        out = nn.ReLU().forward(out)\n",
    "        out = F.linear(out, self.weights_2,self.bias2)\n",
    "        out = nn.ReLU().forward(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,embedding_units,layer_1_n,grapheme_shape,encoder):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.e = encoder\n",
    "        self.embedding_units=embedding_units\n",
    "        self.layer_1_n = layer_1_n\n",
    "\n",
    "        self.weights_layer1 = nn.Parameter(torch.empty(size=(self.layer_1_n, self.embedding_units*2), requires_grad=True))\n",
    "        nn.init.normal_(self.weights_layer1)\n",
    "        self.bias_layer1 = nn.Parameter(torch.zeros(layer_1_n), requires_grad=True)\n",
    "\n",
    "        self.weights_output = nn.Parameter(torch.empty(size=(grapheme_shape, self.layer_1_n), requires_grad=True))\n",
    "        nn.init.normal_(self.weights_output)\n",
    "        self.bias = nn.Parameter(torch.zeros(grapheme_shape), requires_grad=True)\n",
    "        \n",
    "      \n",
    "    def forward(self,inputs):\n",
    "        embedding1 = self.e.forward(inputs[:,0])\n",
    "        embedding2 = self.e.forward(inputs[:,1])\n",
    "\n",
    "        out = torch.cat((embedding1,embedding2),dim=1)\n",
    "\n",
    "        out = nn.ReLU().forward(F.linear(out, self.weights_layer1, self.bias_layer1))\n",
    "\n",
    "        out = nn.ReLU().forward(F.linear(out, self.weights_output, self.bias))\n",
    "   \n",
    "        return out\n",
    "    \n",
    "    def encode(self,inputs):\n",
    "        return self.e.forward(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.80\n",
    "batch_size = 256\n",
    "\n",
    "trn_n = int(x_data.shape[0] * 0.8)\n",
    "batches = (int(trn_n/batch_size))\n",
    "\n",
    "x_trn = x_data[:trn_n,:,:].clone()\n",
    "y_trn = y_data[:trn_n,:].clone()\n",
    "\n",
    "x_val = x_data[trn_n:,:,:].clone()\n",
    "y_val = y_data[trn_n:,:].clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0\n",
      "Validation Loss: 4.515993118286133\n",
      "Train Loss:      7.811649993158156\n",
      "\n",
      "\n",
      "Epoch:1\n",
      "Validation Loss: 3.2581193447113037\n",
      "Train Loss:      3.7171113629494945\n",
      "\n",
      "\n",
      "Epoch:2\n",
      "Validation Loss: 2.9559948444366455\n",
      "Train Loss:      3.074160237466135\n",
      "\n",
      "\n",
      "Epoch:3\n",
      "Validation Loss: 2.7634263038635254\n",
      "Train Loss:      2.8493814760638823\n",
      "\n",
      "\n",
      "Epoch:4\n",
      "Validation Loss: 2.615800380706787\n",
      "Train Loss:      2.66682935683958\n",
      "\n",
      "\n",
      "Epoch:5\n",
      "Validation Loss: 2.562852144241333\n",
      "Train Loss:      2.57461411106971\n",
      "\n",
      "\n",
      "Epoch:6\n",
      "Validation Loss: 2.536576271057129\n",
      "Train Loss:      2.5389042423617454\n",
      "\n",
      "\n",
      "Epoch:7\n",
      "Validation Loss: 2.5192904472351074\n",
      "Train Loss:      2.5177138328552244\n",
      "\n",
      "\n",
      "Epoch:8\n",
      "Validation Loss: 2.5060296058654785\n",
      "Train Loss:      2.5030304678024784\n",
      "\n",
      "\n",
      "Epoch:9\n",
      "Validation Loss: 2.4911348819732666\n",
      "Train Loss:      2.490070893687587\n",
      "\n",
      "\n",
      "Epoch:10\n",
      "Validation Loss: 2.4796462059020996\n",
      "Train Loss:      2.4772941220191216\n",
      "\n",
      "\n",
      "Epoch:11\n",
      "Validation Loss: 2.467043161392212\n",
      "Train Loss:      2.4661033107388404\n",
      "\n",
      "\n",
      "Epoch:12\n",
      "Validation Loss: 2.4561822414398193\n",
      "Train Loss:      2.454403078940607\n",
      "\n",
      "\n",
      "Epoch:13\n",
      "Validation Loss: 2.448075532913208\n",
      "Train Loss:      2.4446975877208095\n",
      "\n",
      "\n",
      "Epoch:14\n",
      "Validation Loss: 2.4406001567840576\n",
      "Train Loss:      2.4368682492163876\n",
      "\n",
      "\n",
      "Epoch:15\n",
      "Validation Loss: 2.4339263439178467\n",
      "Train Loss:      2.429700545341738\n",
      "\n",
      "\n",
      "Epoch:16\n",
      "Validation Loss: 2.4279048442840576\n",
      "Train Loss:      2.423067117506458\n",
      "\n",
      "\n",
      "Epoch:17\n",
      "Validation Loss: 2.4205751419067383\n",
      "Train Loss:      2.4160994452814903\n",
      "\n",
      "\n",
      "Epoch:18\n",
      "Validation Loss: 2.4137868881225586\n",
      "Train Loss:      2.408874705529982\n",
      "\n",
      "\n",
      "Epoch:19\n",
      "Validation Loss: 2.406503200531006\n",
      "Train Loss:      2.4015415499287265\n",
      "\n",
      "\n",
      "Epoch:20\n",
      "Validation Loss: 2.3973143100738525\n",
      "Train Loss:      2.3916542945369597\n",
      "\n",
      "\n",
      "Epoch:21\n",
      "Validation Loss: 2.3890764713287354\n",
      "Train Loss:      2.3819395742108744\n",
      "\n",
      "\n",
      "Epoch:22\n",
      "Validation Loss: 2.380856990814209\n",
      "Train Loss:      2.3735058922921457\n",
      "\n",
      "\n",
      "Epoch:23\n",
      "Validation Loss: 2.3735644817352295\n",
      "Train Loss:      2.36524308881452\n",
      "\n",
      "\n",
      "Epoch:24\n",
      "Validation Loss: 2.36598539352417\n",
      "Train Loss:      2.3578378046712567\n",
      "\n",
      "\n",
      "Epoch:25\n",
      "Validation Loss: 2.359302520751953\n",
      "Train Loss:      2.351108523338072\n",
      "\n",
      "\n",
      "Epoch:26\n",
      "Validation Loss: 2.3533573150634766\n",
      "Train Loss:      2.3452239498015373\n",
      "\n",
      "\n",
      "Epoch:27\n",
      "Validation Loss: 2.3479769229888916\n",
      "Train Loss:      2.340087806024859\n",
      "\n",
      "\n",
      "Epoch:28\n",
      "Validation Loss: 2.3429176807403564\n",
      "Train Loss:      2.3353600025177004\n",
      "\n",
      "\n",
      "Epoch:29\n",
      "Validation Loss: 2.3371963500976562\n",
      "Train Loss:      2.330423104378485\n",
      "\n",
      "\n",
      "Epoch:30\n",
      "Validation Loss: 2.333437442779541\n",
      "Train Loss:      2.3261031612273184\n",
      "\n",
      "\n",
      "Epoch:31\n",
      "Validation Loss: 2.3287906646728516\n",
      "Train Loss:      2.32147828378985\n",
      "\n",
      "\n",
      "Epoch:32\n",
      "Validation Loss: 2.3257229328155518\n",
      "Train Loss:      2.317511664667437\n",
      "\n",
      "\n",
      "Epoch:33\n",
      "Validation Loss: 2.3224728107452393\n",
      "Train Loss:      2.314223663268551\n",
      "\n",
      "\n",
      "Epoch:34\n",
      "Validation Loss: 2.3195252418518066\n",
      "Train Loss:      2.311150280121834\n",
      "\n",
      "\n",
      "Epoch:35\n",
      "Validation Loss: 2.316253423690796\n",
      "Train Loss:      2.3085316042746267\n",
      "\n",
      "\n",
      "Epoch:36\n",
      "Validation Loss: 2.313004493713379\n",
      "Train Loss:      2.3057388444100657\n",
      "\n",
      "\n",
      "Epoch:37\n",
      "Validation Loss: 2.310582160949707\n",
      "Train Loss:      2.303263202790291\n",
      "\n",
      "\n",
      "Epoch:38\n",
      "Validation Loss: 2.308182954788208\n",
      "Train Loss:      2.3011273722494803\n",
      "\n",
      "\n",
      "Epoch:39\n",
      "Validation Loss: 2.3061485290527344\n",
      "Train Loss:      2.2989904326777304\n",
      "\n",
      "\n",
      "Epoch:40\n",
      "Validation Loss: 2.3044188022613525\n",
      "Train Loss:      2.2970411408332088\n",
      "\n",
      "\n",
      "Epoch:41\n",
      "Validation Loss: 2.302485466003418\n",
      "Train Loss:      2.295263338088989\n",
      "\n",
      "\n",
      "Epoch:42\n",
      "Validation Loss: 2.300708293914795\n",
      "Train Loss:      2.293512268989317\n",
      "\n",
      "\n",
      "Epoch:43\n",
      "Validation Loss: 2.2994225025177\n",
      "Train Loss:      2.2917711119497977\n",
      "\n",
      "\n",
      "Epoch:44\n",
      "Validation Loss: 2.2981107234954834\n",
      "Train Loss:      2.290285348892212\n",
      "\n",
      "\n",
      "Epoch:45\n",
      "Validation Loss: 2.2967798709869385\n",
      "Train Loss:      2.2888050725383144\n",
      "\n",
      "\n",
      "Epoch:46\n",
      "Validation Loss: 2.2957544326782227\n",
      "Train Loss:      2.2874813495143766\n",
      "\n",
      "\n",
      "Epoch:47\n",
      "Validation Loss: 2.2946481704711914\n",
      "Train Loss:      2.2862914823716687\n",
      "\n",
      "\n",
      "Epoch:48\n",
      "Validation Loss: 2.2935476303100586\n",
      "Train Loss:      2.285114894374724\n",
      "\n",
      "\n",
      "Epoch:49\n",
      "Validation Loss: 2.2924704551696777\n",
      "Train Loss:      2.2840675000221498\n",
      "\n",
      "\n",
      "Epoch:50\n",
      "Validation Loss: 2.2915239334106445\n",
      "Train Loss:      2.2830653436722295\n",
      "\n",
      "\n",
      "Epoch:51\n",
      "Validation Loss: 2.2907910346984863\n",
      "Train Loss:      2.2821023679548693\n",
      "\n",
      "\n",
      "Epoch:52\n",
      "Validation Loss: 2.2900664806365967\n",
      "Train Loss:      2.2811840257337015\n",
      "\n",
      "\n",
      "Epoch:53\n",
      "Validation Loss: 2.289340019226074\n",
      "Train Loss:      2.280226170632147\n",
      "\n",
      "\n",
      "Epoch:54\n",
      "Validation Loss: 2.2873661518096924\n",
      "Train Loss:      2.2792234959140902\n",
      "\n",
      "\n",
      "Epoch:55\n",
      "Validation Loss: 2.2859034538269043\n",
      "Train Loss:      2.277913296607233\n",
      "\n",
      "\n",
      "Epoch:56\n",
      "Validation Loss: 2.284209728240967\n",
      "Train Loss:      2.276688955676171\n",
      "\n",
      "\n",
      "Epoch:57\n",
      "Validation Loss: 2.282135486602783\n",
      "Train Loss:      2.2753708808652817\n",
      "\n",
      "\n",
      "Epoch:58\n",
      "Validation Loss: 2.2802889347076416\n",
      "Train Loss:      2.2739347965486587\n",
      "\n",
      "\n",
      "Epoch:59\n",
      "Validation Loss: 2.2787511348724365\n",
      "Train Loss:      2.2722635422983477\n",
      "\n",
      "\n",
      "Epoch:60\n",
      "Validation Loss: 2.276956796646118\n",
      "Train Loss:      2.2706703908981813\n",
      "\n",
      "\n",
      "Epoch:61\n",
      "Validation Loss: 2.2753143310546875\n",
      "Train Loss:      2.2687965992958317\n",
      "\n",
      "\n",
      "Epoch:62\n",
      "Validation Loss: 2.2740414142608643\n",
      "Train Loss:      2.267378254859678\n",
      "\n",
      "\n",
      "Epoch:63\n",
      "Validation Loss: 2.272481918334961\n",
      "Train Loss:      2.2660044393231793\n",
      "\n",
      "\n",
      "Epoch:64\n",
      "Validation Loss: 2.270977020263672\n",
      "Train Loss:      2.2646108750374085\n",
      "\n",
      "\n",
      "Epoch:65\n",
      "Validation Loss: 2.2697272300720215\n",
      "Train Loss:      2.263321250484836\n",
      "\n",
      "\n",
      "Epoch:66\n",
      "Validation Loss: 2.268334150314331\n",
      "Train Loss:      2.2621244307487243\n",
      "\n",
      "\n",
      "Epoch:67\n",
      "Validation Loss: 2.26712965965271\n",
      "Train Loss:      2.2608387808645927\n",
      "\n",
      "\n",
      "Epoch:68\n",
      "Validation Loss: 2.266385078430176\n",
      "Train Loss:      2.259806034641881\n",
      "\n",
      "\n",
      "Epoch:69\n",
      "Validation Loss: 2.264878511428833\n",
      "Train Loss:      2.258768852295414\n",
      "\n",
      "\n",
      "Epoch:70\n",
      "Validation Loss: 2.2639431953430176\n",
      "Train Loss:      2.257688247003863\n",
      "\n",
      "\n",
      "Epoch:71\n",
      "Validation Loss: 2.2619991302490234\n",
      "Train Loss:      2.2565286097988007\n",
      "\n",
      "\n",
      "Epoch:72\n",
      "Validation Loss: 2.260401964187622\n",
      "Train Loss:      2.2553397563195996\n",
      "\n",
      "\n",
      "Epoch:73\n",
      "Validation Loss: 2.25888729095459\n",
      "Train Loss:      2.2540888494060884\n",
      "\n",
      "\n",
      "Epoch:74\n",
      "Validation Loss: 2.257798433303833\n",
      "Train Loss:      2.2529656164107785\n",
      "\n",
      "\n",
      "Epoch:75\n",
      "Validation Loss: 2.2567965984344482\n",
      "Train Loss:      2.251946884585965\n",
      "\n",
      "\n",
      "Epoch:76\n",
      "Validation Loss: 2.2556817531585693\n",
      "Train Loss:      2.2509703636169434\n",
      "\n",
      "\n",
      "Epoch:77\n",
      "Validation Loss: 2.2549362182617188\n",
      "Train Loss:      2.2499173072076615\n",
      "\n",
      "\n",
      "Epoch:78\n",
      "Validation Loss: 2.253985643386841\n",
      "Train Loss:      2.2489308172656646\n",
      "\n",
      "\n",
      "Epoch:79\n",
      "Validation Loss: 2.253472089767456\n",
      "Train Loss:      2.248061450835197\n",
      "\n",
      "\n",
      "Epoch:80\n",
      "Validation Loss: 2.252358913421631\n",
      "Train Loss:      2.2471488291217434\n",
      "\n",
      "\n",
      "Epoch:81\n",
      "Validation Loss: 2.2516515254974365\n",
      "Train Loss:      2.2462590432936147\n",
      "\n",
      "\n",
      "Epoch:82\n",
      "Validation Loss: 2.251263380050659\n",
      "Train Loss:      2.245474979954381\n",
      "\n",
      "\n",
      "Epoch:83\n",
      "Validation Loss: 2.250168800354004\n",
      "Train Loss:      2.2445934480236422\n",
      "\n",
      "\n",
      "Epoch:84\n",
      "Validation Loss: 2.2491073608398438\n",
      "Train Loss:      2.2437039159959364\n",
      "\n",
      "\n",
      "Epoch:85\n",
      "Validation Loss: 2.2478156089782715\n",
      "Train Loss:      2.2428903748912195\n",
      "\n",
      "\n",
      "Epoch:86\n",
      "Validation Loss: 2.2470316886901855\n",
      "Train Loss:      2.2418324055210235\n",
      "\n",
      "\n",
      "Epoch:87\n",
      "Validation Loss: 2.2462730407714844\n",
      "Train Loss:      2.2409223218117993\n",
      "\n",
      "\n",
      "Epoch:88\n",
      "Validation Loss: 2.244779586791992\n",
      "Train Loss:      2.240147218396587\n",
      "\n",
      "\n",
      "Epoch:89\n",
      "Validation Loss: 2.2441821098327637\n",
      "Train Loss:      2.2394318272990565\n",
      "\n",
      "\n",
      "Epoch:90\n",
      "Validation Loss: 2.2433857917785645\n",
      "Train Loss:      2.238726108304916\n",
      "\n",
      "\n",
      "Epoch:91\n",
      "Validation Loss: 2.242738962173462\n",
      "Train Loss:      2.2380414316731114\n",
      "\n",
      "\n",
      "Epoch:92\n",
      "Validation Loss: 2.2422142028808594\n",
      "Train Loss:      2.23749908016574\n",
      "\n",
      "\n",
      "Epoch:93\n",
      "Validation Loss: 2.241691827774048\n",
      "Train Loss:      2.236860519839871\n",
      "\n",
      "\n",
      "Epoch:94\n",
      "Validation Loss: 2.2410635948181152\n",
      "Train Loss:      2.2363008452999975\n",
      "\n",
      "\n",
      "Epoch:95\n",
      "Validation Loss: 2.2403359413146973\n",
      "Train Loss:      2.235804833135297\n",
      "\n",
      "\n",
      "Epoch:96\n",
      "Validation Loss: 2.237469434738159\n",
      "Train Loss:      2.2329479417493268\n",
      "\n",
      "\n",
      "Epoch:97\n",
      "Validation Loss: 2.236205816268921\n",
      "Train Loss:      2.23182692527771\n",
      "\n",
      "\n",
      "Epoch:98\n",
      "Validation Loss: 2.2355878353118896\n",
      "Train Loss:      2.2305392496047483\n",
      "\n",
      "\n",
      "Epoch:99\n",
      "Validation Loss: 2.2346856594085693\n",
      "Train Loss:      2.2297603499504826\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metric_dict = {'losses_trn': [],'losses_val':[]} \n",
    "encoder =  Encoder(grapheme_shape=v,hidden_units=10,embedding_units=6)\n",
    "decoder = Decoder(embedding_units=6,layer_1_n=15,grapheme_shape=v,encoder=encoder)\n",
    "optimizer = optim.Adam(decoder.parameters(), amsgrad=False, weight_decay=0.0)\n",
    "\n",
    "trn_loss = nn.NLLLoss(weight=freqs.float())\n",
    "val_loss = nn.NLLLoss()\n",
    "\n",
    "for epoch in range(100):\n",
    "    print(\"Epoch:\" + str(epoch))\n",
    "    epoch_trn_loss_sum = 0\n",
    "    for batch in (range(batches)):\n",
    "        x_batch = x_trn[batch_size*batch:batch_size*(batch+1)]\n",
    "        y_batch = y_trn[batch_size*batch:batch_size*(batch+1)]\n",
    "        \n",
    "        out = decoder.forward(x_batch)\n",
    "        log_softmax = F.log_softmax(out, dim=1).unsqueeze(2)\n",
    "        \n",
    "       \n",
    "        \n",
    "        NLL_Loss = trn_loss(log_softmax,y_batch.long()) \n",
    "        optimizer.zero_grad()\n",
    "        NLL_Loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_trn_loss_sum += NLL_Loss.item()\n",
    "        \n",
    "    out_val = decoder.forward(x_val)\n",
    "    log_val_softmax = F.log_softmax(out_val, dim=1).unsqueeze(2)\n",
    "    NLL_val_loss = trn_loss(log_val_softmax, y_val.long())\n",
    "    \n",
    "    epoch_trn_loss = epoch_trn_loss_sum/batches\n",
    "    print(\"Validation Loss: \"+str(NLL_val_loss.item()))\n",
    "    print(\"Train Loss:      \"+str(epoch_trn_loss))\n",
    "    print(\"\\n\")\n",
    "   # print(\"Val: \"+str(val_loss.item()))\n",
    "    metric_dict['losses_trn'].append(NLL_Loss.item())\n",
    "    metric_dict['losses_val'].append(epoch_trn_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ffe1fc3cf10>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(metric_dict['losses_trn'][:100])\n",
    "plt.plot(metric_dict['losses_val'][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = np.zeros(shape=(len(vocab),10))\n",
    "\n",
    "i=0\n",
    "for grapheme in vocab:\n",
    "    test = torch.from_numpy(graph2vec(grapheme)).float()\n",
    "    embedding = decoder.encode(test).detach().numpy()\n",
    "    embeddings[i,:]=embedding\n",
    "    i+=1\n",
    "\n",
    "\n",
    "\n",
    "embedding_means = embeddings.mean(axis=0)\n",
    "embedding_std = embeddings.std(axis=0)\n",
    "norm_embeddings = (embeddings-embedding_means)/embedding_std\n",
    "\n",
    "    \n",
    "covariance_matrix = np.cov(norm_embeddings.T)\n",
    "v,w = np.linalg.eig(covariance_matrix)\n",
    "\n",
    "idx = v.argsort()[::-1] # Sort descending and get sorted indices\n",
    "v = v[idx] # Use indices on eigv vector\n",
    "w = w[:,idx] # \n",
    "\n",
    "variance_explained = []\n",
    "for i in v:\n",
    "     variance_explained.append((i/sum(v))*100)\n",
    "        \n",
    "red_Vecs = w[0:2,:]\n",
    "\n",
    "low_d_embed = (embeddings @ red_Vecs.T)\n",
    "\n",
    "#plt.scatter(x=low_d_embed[:,0],y=low_d_embed[:,1])\n",
    "\n",
    "x=low_d_embed[:,0]\n",
    "y=low_d_embed[:,1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y,color='white')\n",
    "\n",
    "for i, txt in enumerate(vocab):\n",
    "    ax.annotate(txt, (x[i], y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n', 'g']\n",
      "Prediction: ng e\n",
      "True:       ng £\n"
     ]
    }
   ],
   "source": [
    "n=randint(0,x_trn.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "input = x_trn[n,:,:].unsqueeze(0)\n",
    "output = y_trn[n,:]\n",
    "\n",
    "out=decoder.forward(input)\n",
    "log_softmax = -F.log_softmax(out, dim=1)\n",
    "\n",
    "p=(log_softmax.argmin(dim=1).detach().numpy()[0])\n",
    "o=(output.int().numpy())\n",
    "\n",
    "numpy_input = input.int().detach().numpy()[0]\n",
    "inp_ind = (numpy_input.argmax(axis=1))\n",
    "inp_let = [vocab[i] for i in inp_ind]\n",
    "\n",
    "\n",
    "print(inp_let)\n",
    "print(\"Prediction: \" + ''.join(inp_let)+\" \"+str(vocab[p]))\n",
    "print(\"True:       \" + ''.join(inp_let)+\" \"+str(vocab[o[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$', '$', 'w', 'e', 'i', 'ß', 'l', 'i', 'c', 'h', 'e', 'm']\n",
      "['h', 'e']\n",
      "33\n",
      "['h', 'e']\n",
      "Prediction: $$weißliche n\n",
      "True:       $$weißliche m\n"
     ]
    }
   ],
   "source": [
    "n=randint(0,len(tokens))\n",
    "token=(tokens[n])[:-1]\n",
    "print(token)\n",
    "\n",
    "input = token[-3:-1]\n",
    "\n",
    "#input = x_trn[n,:,:].unsqueeze(0)\n",
    "output = token[-1:][0]\n",
    "\n",
    "input = torch.tensor([graph2vec(input[0]),graph2vec(input[1])]).unsqueeze(0).float()\n",
    "\n",
    "out=decoder.forward(input)\n",
    "log_softmax = -F.log_softmax(out, dim=1)\n",
    "\n",
    "p=(log_softmax.argmin(dim=1).detach().numpy()[0])\n",
    "o=(output)\n",
    "\n",
    "numpy_input = input.int().detach().numpy()[0]\n",
    "inp_ind = (numpy_input.argmax(axis=1))\n",
    "inp_let = [vocab[i] for i in inp_ind]\n",
    "print(inp_let)\n",
    "print(p)\n",
    "\n",
    "\n",
    "print(inp_let)\n",
    "print(\"Prediction: \" + ''.join(token[:-1])+\" \"+str(vocab[p]))\n",
    "print(\"True:       \" + ''.join(token[:-1])+\" \"+str(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secetes lapdatite rensss uus cawr slbryens inasts wequimtidesl ue sisnd\n"
     ]
    }
   ],
   "source": [
    "n=randint(0,len(tokens))\n",
    "token=(tokens[n])[:-1]\n",
    "sentence = []\n",
    "\n",
    "for i in range(10):\n",
    "    string = \"$$\"\n",
    "\n",
    "    while True:\n",
    "\n",
    "        input=string[-2:]\n",
    "        input = torch.tensor([graph2vec(input[0]),graph2vec(input[1])]).unsqueeze(0).float()\n",
    "\n",
    "        out=decoder.forward(input)\n",
    "        probs = F.softmax(out, dim=1).detach().numpy()[0]\n",
    "\n",
    "        sample = np.random.multinomial(1, probs)\n",
    "        ids = np.argmax(sample)\n",
    "\n",
    "        #p=(log_softmax.argmin(dim=1).detach().numpy()[0])\n",
    "\n",
    "        grapheme = str(vocab[ids])\n",
    "\n",
    "        string+=grapheme\n",
    "        if grapheme ==\"£\":\n",
    "            break\n",
    "\n",
    "    string = string.split('$')[2:][0]\n",
    "    string = string.split('£')[:-1][0]\n",
    "    sentence.append(string)\n",
    "\n",
    "print(' '.join(sentence))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbour of a : c\n",
      "Nearest neighbour of b : p\n",
      "Nearest neighbour of c : f\n",
      "Nearest neighbour of d : z\n",
      "Nearest neighbour of e : s\n",
      "Nearest neighbour of f : x\n",
      "Nearest neighbour of g : q\n",
      "Nearest neighbour of h : r\n",
      "Nearest neighbour of i : k\n",
      "Nearest neighbour of j : w\n",
      "Nearest neighbour of k : i\n",
      "Nearest neighbour of l : d\n",
      "Nearest neighbour of m : x\n",
      "Nearest neighbour of n : r\n",
      "Nearest neighbour of o : a\n",
      "Nearest neighbour of p : b\n",
      "Nearest neighbour of q : s\n",
      "Nearest neighbour of r : n\n",
      "Nearest neighbour of s : t\n",
      "Nearest neighbour of t : s\n",
      "Nearest neighbour of u : b\n",
      "Nearest neighbour of v : $\n",
      "Nearest neighbour of w : $\n",
      "Nearest neighbour of x : f\n",
      "Nearest neighbour of y : t\n",
      "Nearest neighbour of z : £\n",
      "Nearest neighbour of $ : v\n",
      "Nearest neighbour of £ : z\n"
     ]
    }
   ],
   "source": [
    "embeddings = np.zeros(shape=(len(vocab),10))\n",
    "\n",
    "i=0\n",
    "for grapheme in vocab:\n",
    "    test = torch.from_numpy(graph2vec(grapheme)).float()\n",
    "    embedding = decoder.encode(test).detach().numpy()\n",
    "    embeddings[i,:]=embedding\n",
    "    i+=1\n",
    "    \n",
    "l=0\n",
    "for e in (embeddings):\n",
    "    dists=(np.square(embeddings-e).sum(axis=1))\n",
    "    nearest_idx=(np.argsort(dists)[1])\n",
    "    print(\"Nearest neighbour of \"+vocab[l]+\" : \"+vocab[nearest_idx])\n",
    "    l+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
