{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grapheme Embedding Using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import tqdm\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"graphemes.txt\")\n",
    "processed_text = (re.sub(r'[^\\w\\s]','',text.read().lower()))\n",
    "\n",
    "vocab = list('abcdefghijklmnopqrstuvwxyz$£')\n",
    "\n",
    "tokens = [list(\"$$\"+token+\"£\") for token in processed_text.split('\\n') if set(token)&set(vocab)==set(token)]\n",
    "\n",
    "\n",
    "v = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "en\n"
     ]
    }
   ],
   "source": [
    "pairFreqs={}\n",
    "replacements = {}\n",
    "\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    tempTokens = []\n",
    "    for token in tokens:\n",
    "        if i!=0:\n",
    "            token = list(''.join(token).replace(topOc,str(i-1)))\n",
    "        for g in range(2,len(token)-2):\n",
    "            pair = (token[g]+token[g+1])\n",
    "            if pair in pairFreqs.keys():\n",
    "                pairFreqs[pair] += 1\n",
    "            else:\n",
    "                pairFreqs[pair] = 1\n",
    "        tempTokens.append(token)\n",
    "        \n",
    "    tokens = tempTokens\n",
    "    topOcs = [(k, v) for k, v in sorted(pairFreqs.items(), key=lambda item: item[1],reverse=True)]\n",
    "    topOc = (topOcs[0][0])\n",
    "    \n",
    "    replacements[i] = topOc\n",
    "print(topOc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'er',\n",
       " 1: 'in',\n",
       " 2: 'es',\n",
       " 3: 'es',\n",
       " 4: 'on',\n",
       " 5: 'at',\n",
       " 6: 'an',\n",
       " 7: 'ed',\n",
       " 8: 'is',\n",
       " 9: 'en'}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph2int(graph):\n",
    "    return vocab.index(graph)\n",
    "    \n",
    "def int2graph(index):\n",
    "    return vocab[index]\n",
    "\n",
    "def int2vec(integer):\n",
    "    vec=torch.zeros(len(vocab))\n",
    "    vec[integer]=1\n",
    "    return vec\n",
    "\n",
    "def graph2vec(graph):\n",
    "    return (int2vec(graph2int(graph)))\n",
    "\n",
    "def vec2graph(vec):\n",
    "    return (int2graph(np.argmax(vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSize = 2\n",
    "tokens = [tokenSet for tokenSet in tokens if len(tokenSet)>=6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 13515/75839 [15:42<1:12:28, 14.33it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-376-6f2fc7d0bb79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mgraph_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwc\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mdata_sample\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph_i\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mx_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mlabel_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph2int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph_i\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0my_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_sample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_data = torch.empty(size=(1,2,28))\n",
    "y_data = torch.empty(size=(1,1))\n",
    "\n",
    "freqs= torch.zeros(size=(1,28))\n",
    "\n",
    "i=0\n",
    "N=len(tokens)\n",
    "for word in tqdm.tqdm(tokens):\n",
    "    wc = len(word)\n",
    "    for graph_i in range(wc-2): \n",
    "        data_sample =torch.stack((graph2vec(word[graph_i]), graph2vec(word[graph_i+1]))).unsqueeze(0)\n",
    "        x_data = torch.cat((x_data,data_sample),0)\n",
    "        label_sample = torch.tensor([graph2int(word[graph_i+2])]).unsqueeze(0).float()\n",
    "        y_data = torch.cat([y_data,label_sample])\n",
    "        freqs += graph2vec(word[graph_i+2])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data=torch.load('EnglishData.pt')\n",
    "y_data=torch.load('EnglishLabels.pt')\n",
    "freqs=torch.load('EnglishFreqs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([129814, 2, 28])\n",
      "torch.Size([129814, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 9178.,  2287.,  4582.,  4086., 13151.,  1543.,  3029.,  2678.,  9765.,\n",
       "           244.,  1084.,  6388.,  3181.,  7922.,  7128.,  3158.,   197.,  8997.,\n",
       "         11110.,  7605.,  4113.,  1152.,  1065.,   309.,  1857.,   490.,     0.,\n",
       "         13515.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_data.shape)\n",
    "print(y_data.shape)\n",
    "freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,grapheme_shape,hidden_units,embedding_units):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.word_shape = grapheme_shape\n",
    "        self.hidden_units = hidden_units\n",
    "        self.embedding_units = embedding_units\n",
    "        \n",
    "        self.weights_1 = nn.Parameter(torch.empty(size=(hidden_units, grapheme_shape), requires_grad=True))\n",
    "        nn.init.normal_(self.weights_1)\n",
    "        \n",
    "        self.weights_2 = nn.Parameter(torch.empty(size=(embedding_units, hidden_units), requires_grad=True))\n",
    "        nn.init.normal_(self.weights_2)\n",
    "        \n",
    "        self.bias1 = nn.Parameter(torch.zeros(hidden_units), requires_grad=True)\n",
    "        self.bias2 = nn.Parameter(torch.zeros(embedding_units), requires_grad=True)\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        out = F.linear(inputs, self.weights_1, self.bias1)\n",
    "        out = nn.ReLU().forward(out)\n",
    "        out = F.linear(out, self.weights_2,self.bias2)\n",
    "        out = nn.ReLU().forward(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,embedding_units,hidden_dim,grapheme_shape,encoder):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.e = encoder\n",
    "        self.embedding_units=embedding_units\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.n_layers=2\n",
    "        self.rnn = nn.LSTM(self.embedding_units, hidden_dim,self.n_layers, batch_first=True)   \n",
    "        self.fc = nn.Linear(hidden_dim, grapheme_shape)\n",
    "        self.bias = nn.Parameter(torch.zeros(grapheme_shape), requires_grad=True)\n",
    "        \n",
    "        embedding = torch.randn(256,1,self.embedding_units)\n",
    "\n",
    "        h0 = torch.randn(self.n_layers,256,self.hidden_dim)\n",
    "     \n",
    "        hidden, output = self.rnn.forward(embedding, h0)\n",
    "      \n",
    "    def forward(self,inputs):\n",
    "        #print(inputs.shape)\n",
    "        embedding1 = self.e.forward(inputs[:,0])\n",
    "        embedding2 = self.e.forward(inputs[:,1])\n",
    "        embeddings = torch.stack((embedding1,embedding2),dim=1)\n",
    "        #print(embeddings.shape)\n",
    "        \n",
    "        hidden = torch.randn(self.n_layers,inputs.shape[0], self.hidden_dim)\n",
    "        output,hidden = self.rnn.forward(embeddings,hidden)\n",
    "        \n",
    "        out = self.fc.forward(output)\n",
    "        #print(out.shape)\n",
    "        return out[:,-1,:]\n",
    "    \n",
    "    def encode(self,inputs):\n",
    "        return self.e.forward(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 10])\n",
      "torch.Size([2, 3, 20])\n",
      "torch.Size([5, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "rnn = nn.RNN(10, 20, 2)         # rnn with input size 10, hidden size 20, and 2 rnn layers\n",
    "input = torch.randn(5, 3, 10)   # input 5 batches of shape 3,10\n",
    "h0 = torch.randn(2, 3, 20)      # two rnn layers of shape 3,20\n",
    "\n",
    "print(input.shape)\n",
    "print(h0.shape)\n",
    "\n",
    "output, hn = rnn(input, h0)     # output 5 batches of shape 3,20\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.80\n",
    "batch_size = 256\n",
    "\n",
    "trn_n = int(x_data.shape[0] * 0.8)\n",
    "batches = (int(trn_n/batch_size))\n",
    "\n",
    "x_trn = x_data[:trn_n,:,:].clone()\n",
    "y_trn = y_data[:trn_n,:].clone()\n",
    "\n",
    "x_val = x_data[trn_n:,:,:].clone()\n",
    "y_val = y_data[trn_n:,:].clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden[0] size (2, 256, 20), got (256, 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-c08a1cc81f28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0membed_units\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrapheme_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membedding_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membed_units\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membed_units\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrapheme_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-a843c4eacfd3>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embedding_units, hidden_dim, grapheme_shape, encoder)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mh0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/project/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/project/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         self.check_hidden_size(hidden[0], expected_hidden_size,\n\u001b[0;32m--> 529\u001b[0;31m                                'Expected hidden[0] size {}, got {}')\n\u001b[0m\u001b[1;32m    530\u001b[0m         self.check_hidden_size(hidden[1], expected_hidden_size,\n\u001b[1;32m    531\u001b[0m                                'Expected hidden[1] size {}, got {}')\n",
      "\u001b[0;32m~/opt/anaconda3/envs/project/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_hidden_size\u001b[0;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[1;32m    193\u001b[0m                           msg: str = 'Expected hidden size {}, got {}') -> None:\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected hidden[0] size (2, 256, 20), got (256, 20)"
     ]
    }
   ],
   "source": [
    "metric_dict = {'losses_trn': [],'losses_val':[]} \n",
    "embed_units = 20\n",
    "encoder =  Encoder(grapheme_shape=v,hidden_units=15,embedding_units=embed_units)\n",
    "decoder = Decoder(embedding_units=embed_units,hidden_dim=20,grapheme_shape=v,encoder=encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0\n",
      "Validation Loss: 2.040922164916992\n",
      "Train Loss:      2.1882235594737676\n",
      "\n",
      "\n",
      "Epoch:1\n",
      "Validation Loss: 2.002100706100464\n",
      "Train Loss:      2.0164725889394313\n",
      "\n",
      "\n",
      "Epoch:2\n",
      "Validation Loss: 1.986512303352356\n",
      "Train Loss:      1.9898815852624399\n",
      "\n",
      "\n",
      "Epoch:3\n",
      "Validation Loss: 1.9745442867279053\n",
      "Train Loss:      1.9782092803790245\n",
      "\n",
      "\n",
      "Epoch:4\n",
      "Validation Loss: 1.967623233795166\n",
      "Train Loss:      1.970267387378363\n",
      "\n",
      "\n",
      "Epoch:5\n",
      "Validation Loss: 1.9654531478881836\n",
      "Train Loss:      1.9641549437134354\n",
      "\n",
      "\n",
      "Epoch:6\n",
      "Validation Loss: 1.9614325761795044\n",
      "Train Loss:      1.9598685526553496\n",
      "\n",
      "\n",
      "Epoch:7\n",
      "Validation Loss: 1.9577784538269043\n",
      "Train Loss:      1.954815368004787\n",
      "\n",
      "\n",
      "Epoch:8\n",
      "Validation Loss: 1.9579503536224365\n",
      "Train Loss:      1.9523080755163122\n",
      "\n",
      "\n",
      "Epoch:9\n",
      "Validation Loss: 1.9520522356033325\n",
      "Train Loss:      1.949801895942217\n",
      "\n",
      "\n",
      "Epoch:10\n",
      "Validation Loss: 1.9510823488235474\n",
      "Train Loss:      1.9477024328561476\n",
      "\n",
      "\n",
      "Epoch:11\n",
      "Validation Loss: 1.9516372680664062\n",
      "Train Loss:      1.9458886170092924\n",
      "\n",
      "\n",
      "Epoch:12\n",
      "Validation Loss: 1.9494235515594482\n",
      "Train Loss:      1.9443169267089278\n",
      "\n",
      "\n",
      "Epoch:13\n",
      "Validation Loss: 1.9509989023208618\n",
      "Train Loss:      1.9426890655800149\n",
      "\n",
      "\n",
      "Epoch:14\n",
      "Validation Loss: 1.9438973665237427\n",
      "Train Loss:      1.9414833725234608\n",
      "\n",
      "\n",
      "Epoch:15\n",
      "Validation Loss: 1.947213053703308\n",
      "Train Loss:      1.9414926523043785\n",
      "\n",
      "\n",
      "Epoch:16\n",
      "Validation Loss: 1.9492707252502441\n",
      "Train Loss:      1.9395565848291656\n",
      "\n",
      "\n",
      "Epoch:17\n",
      "Validation Loss: 1.9491310119628906\n",
      "Train Loss:      1.9388122805842647\n",
      "\n",
      "\n",
      "Epoch:18\n",
      "Validation Loss: 1.9491490125656128\n",
      "Train Loss:      1.9389952883308317\n",
      "\n",
      "\n",
      "Epoch:19\n",
      "Validation Loss: 1.947638988494873\n",
      "Train Loss:      1.9379902477617617\n",
      "\n",
      "\n",
      "Epoch:20\n",
      "Validation Loss: 1.9471012353897095\n",
      "Train Loss:      1.9373212899690793\n",
      "\n",
      "\n",
      "Epoch:21\n",
      "Validation Loss: 1.944902777671814\n",
      "Train Loss:      1.9367144602316397\n",
      "\n",
      "\n",
      "Epoch:22\n",
      "Validation Loss: 1.9435924291610718\n",
      "Train Loss:      1.936478771103753\n",
      "\n",
      "\n",
      "Epoch:23\n",
      "Validation Loss: 1.943932056427002\n",
      "Train Loss:      1.9350185985918398\n",
      "\n",
      "\n",
      "Epoch:24\n",
      "Validation Loss: 1.94503653049469\n",
      "Train Loss:      1.9350362877786895\n",
      "\n",
      "\n",
      "Epoch:25\n",
      "Validation Loss: 1.942155361175537\n",
      "Train Loss:      1.9349756058351493\n",
      "\n",
      "\n",
      "Epoch:26\n",
      "Validation Loss: 1.9408398866653442\n",
      "Train Loss:      1.9335717616257844\n",
      "\n",
      "\n",
      "Epoch:27\n",
      "Validation Loss: 1.9398366212844849\n",
      "Train Loss:      1.9337765993895355\n",
      "\n",
      "\n",
      "Epoch:28\n",
      "Validation Loss: 1.9418829679489136\n",
      "Train Loss:      1.9328172633677354\n",
      "\n",
      "\n",
      "Epoch:29\n",
      "Validation Loss: 1.9398982524871826\n",
      "Train Loss:      1.932802645954085\n",
      "\n",
      "\n",
      "Epoch:30\n",
      "Validation Loss: 1.9414509534835815\n",
      "Train Loss:      1.932628505318253\n",
      "\n",
      "\n",
      "Epoch:31\n",
      "Validation Loss: 1.9381306171417236\n",
      "Train Loss:      1.9321516075252014\n",
      "\n",
      "\n",
      "Epoch:32\n",
      "Validation Loss: 1.9438740015029907\n",
      "Train Loss:      1.9311123341689875\n",
      "\n",
      "\n",
      "Epoch:33\n",
      "Validation Loss: 1.9396717548370361\n",
      "Train Loss:      1.930915642079012\n",
      "\n",
      "\n",
      "Epoch:34\n",
      "Validation Loss: 1.9378870725631714\n",
      "Train Loss:      1.9318887342641382\n",
      "\n",
      "\n",
      "Epoch:35\n",
      "Validation Loss: 1.9371483325958252\n",
      "Train Loss:      1.93195522096422\n",
      "\n",
      "\n",
      "Epoch:36\n",
      "Validation Loss: 1.9361556768417358\n",
      "Train Loss:      1.9308154003119764\n",
      "\n",
      "\n",
      "Epoch:37\n",
      "Validation Loss: 1.9384047985076904\n",
      "Train Loss:      1.9302423141620777\n",
      "\n",
      "\n",
      "Epoch:38\n",
      "Validation Loss: 1.9383293390274048\n",
      "Train Loss:      1.931079849196069\n",
      "\n",
      "\n",
      "Epoch:39\n",
      "Validation Loss: 1.9392688274383545\n",
      "Train Loss:      1.9308763839580394\n",
      "\n",
      "\n",
      "Epoch:40\n",
      "Validation Loss: 1.9364020824432373\n",
      "Train Loss:      1.9294421254852672\n",
      "\n",
      "\n",
      "Epoch:41\n",
      "Validation Loss: 1.9377350807189941\n",
      "Train Loss:      1.9290369195702635\n",
      "\n",
      "\n",
      "Epoch:42\n",
      "Validation Loss: 1.9339890480041504\n",
      "Train Loss:      1.9286537644303876\n",
      "\n",
      "\n",
      "Epoch:43\n",
      "Validation Loss: 1.9361950159072876\n",
      "Train Loss:      1.9291677251274204\n",
      "\n",
      "\n",
      "Epoch:44\n",
      "Validation Loss: 1.9366354942321777\n",
      "Train Loss:      1.9288408482516253\n",
      "\n",
      "\n",
      "Epoch:45\n",
      "Validation Loss: 1.9337570667266846\n",
      "Train Loss:      1.9296387637103045\n",
      "\n",
      "\n",
      "Epoch:46\n",
      "Validation Loss: 1.9373517036437988\n",
      "Train Loss:      1.928994928465949\n",
      "\n",
      "\n",
      "Epoch:47\n",
      "Validation Loss: 1.9340457916259766\n",
      "Train Loss:      1.9296341339747112\n",
      "\n",
      "\n",
      "Epoch:48\n",
      "Validation Loss: 1.9330430030822754\n",
      "Train Loss:      1.9284951077567207\n",
      "\n",
      "\n",
      "Epoch:49\n",
      "Validation Loss: 1.933570146560669\n",
      "Train Loss:      1.9289354156564784\n",
      "\n",
      "\n",
      "Epoch:50\n",
      "Validation Loss: 1.9369157552719116\n",
      "Train Loss:      1.9286415500405394\n",
      "\n",
      "\n",
      "Epoch:51\n",
      "Validation Loss: 1.935386300086975\n",
      "Train Loss:      1.9291557153066\n",
      "\n",
      "\n",
      "Epoch:52\n",
      "Validation Loss: 1.9381204843521118\n",
      "Train Loss:      1.9284534186492732\n",
      "\n",
      "\n",
      "Epoch:53\n",
      "Validation Loss: 1.9395060539245605\n",
      "Train Loss:      1.9272147319934987\n",
      "\n",
      "\n",
      "Epoch:54\n",
      "Validation Loss: 1.9361592531204224\n",
      "Train Loss:      1.9284105256751731\n",
      "\n",
      "\n",
      "Epoch:55\n",
      "Validation Loss: 1.9357595443725586\n",
      "Train Loss:      1.9274139006932576\n",
      "\n",
      "\n",
      "Epoch:56\n",
      "Validation Loss: 1.9347141981124878\n",
      "Train Loss:      1.9281423400949549\n",
      "\n",
      "\n",
      "Epoch:57\n",
      "Validation Loss: 1.9354918003082275\n",
      "Train Loss:      1.927204785817935\n",
      "\n",
      "\n",
      "Epoch:58\n",
      "Validation Loss: 1.9350345134735107\n",
      "Train Loss:      1.9260540432400173\n",
      "\n",
      "\n",
      "Epoch:59\n",
      "Validation Loss: 1.9335756301879883\n",
      "Train Loss:      1.928245218300525\n",
      "\n",
      "\n",
      "Epoch:60\n",
      "Validation Loss: 1.9351739883422852\n",
      "Train Loss:      1.927292753443306\n",
      "\n",
      "\n",
      "Epoch:61\n",
      "Validation Loss: 1.9331443309783936\n",
      "Train Loss:      1.9270043340730079\n",
      "\n",
      "\n",
      "Epoch:62\n",
      "Validation Loss: 1.9366183280944824\n",
      "Train Loss:      1.9258608976999918\n",
      "\n",
      "\n",
      "Epoch:63\n",
      "Validation Loss: 1.9340400695800781\n",
      "Train Loss:      1.9263892971439127\n",
      "\n",
      "\n",
      "Epoch:64\n",
      "Validation Loss: 1.9319647550582886\n",
      "Train Loss:      1.927089571364132\n",
      "\n",
      "\n",
      "Epoch:65\n",
      "Validation Loss: 1.9372897148132324\n",
      "Train Loss:      1.9262462197998425\n",
      "\n",
      "\n",
      "Epoch:66\n",
      "Validation Loss: 1.9329118728637695\n",
      "Train Loss:      1.9267266965206757\n",
      "\n",
      "\n",
      "Epoch:67\n",
      "Validation Loss: 1.9367650747299194\n",
      "Train Loss:      1.9252316990016418\n",
      "\n",
      "\n",
      "Epoch:68\n",
      "Validation Loss: 1.9317474365234375\n",
      "Train Loss:      1.9261312626026295\n",
      "\n",
      "\n",
      "Epoch:69\n",
      "Validation Loss: 1.9322127103805542\n",
      "Train Loss:      1.9278241292929943\n",
      "\n",
      "\n",
      "Epoch:70\n",
      "Validation Loss: 1.9326120615005493\n",
      "Train Loss:      1.926469670401679\n",
      "\n",
      "\n",
      "Epoch:71\n",
      "Validation Loss: 1.9331923723220825\n",
      "Train Loss:      1.9268014510472615\n",
      "\n",
      "\n",
      "Epoch:72\n",
      "Validation Loss: 1.932720422744751\n",
      "Train Loss:      1.9261975841757693\n",
      "\n",
      "\n",
      "Epoch:73\n",
      "Validation Loss: 1.9330759048461914\n",
      "Train Loss:      1.9263091125606018\n",
      "\n",
      "\n",
      "Epoch:74\n",
      "Validation Loss: 1.933464765548706\n",
      "Train Loss:      1.925855792304616\n",
      "\n",
      "\n",
      "Epoch:75\n",
      "Validation Loss: 1.9324419498443604\n",
      "Train Loss:      1.925732162852346\n",
      "\n",
      "\n",
      "Epoch:76\n",
      "Validation Loss: 1.932694673538208\n",
      "Train Loss:      1.92517294206737\n",
      "\n",
      "\n",
      "Epoch:77\n",
      "Validation Loss: 1.937748908996582\n",
      "Train Loss:      1.9253900681012943\n",
      "\n",
      "\n",
      "Epoch:78\n",
      "Validation Loss: 1.9320735931396484\n",
      "Train Loss:      1.925796029302809\n",
      "\n",
      "\n",
      "Epoch:79\n",
      "Validation Loss: 1.9325767755508423\n",
      "Train Loss:      1.9258031636108588\n",
      "\n",
      "\n",
      "Epoch:80\n",
      "Validation Loss: 1.9340604543685913\n",
      "Train Loss:      1.9261041461685557\n",
      "\n",
      "\n",
      "Epoch:81\n",
      "Validation Loss: 1.935369610786438\n",
      "Train Loss:      1.9266934235890707\n",
      "\n",
      "\n",
      "Epoch:82\n",
      "Validation Loss: 1.930590033531189\n",
      "Train Loss:      1.9255668425265653\n",
      "\n",
      "\n",
      "Epoch:83\n",
      "Validation Loss: 1.9303297996520996\n",
      "Train Loss:      1.9260255595784128\n",
      "\n",
      "\n",
      "Epoch:84\n",
      "Validation Loss: 1.9320731163024902\n",
      "Train Loss:      1.9248495781863177\n",
      "\n",
      "\n",
      "Epoch:85\n",
      "Validation Loss: 1.9314016103744507\n",
      "Train Loss:      1.9247278878718246\n",
      "\n",
      "\n",
      "Epoch:86\n",
      "Validation Loss: 1.931624174118042\n",
      "Train Loss:      1.924896846877204\n",
      "\n",
      "\n",
      "Epoch:87\n",
      "Validation Loss: 1.9328029155731201\n",
      "Train Loss:      1.9246404453560157\n",
      "\n",
      "\n",
      "Epoch:88\n",
      "Validation Loss: 1.9345782995224\n",
      "Train Loss:      1.9251459383670195\n",
      "\n",
      "\n",
      "Epoch:89\n",
      "Validation Loss: 1.9340282678604126\n",
      "Train Loss:      1.923945919672648\n",
      "\n",
      "\n",
      "Epoch:90\n",
      "Validation Loss: 1.9338003396987915\n",
      "Train Loss:      1.9251339618070626\n",
      "\n",
      "\n",
      "Epoch:91\n",
      "Validation Loss: 1.933703899383545\n",
      "Train Loss:      1.9255414229852181\n",
      "\n",
      "\n",
      "Epoch:92\n",
      "Validation Loss: 1.9357163906097412\n",
      "Train Loss:      1.9250357177522448\n",
      "\n",
      "\n",
      "Epoch:93\n",
      "Validation Loss: 1.9335687160491943\n",
      "Train Loss:      1.9246520357367434\n",
      "\n",
      "\n",
      "Epoch:94\n",
      "Validation Loss: 1.9317835569381714\n",
      "Train Loss:      1.9244408913600592\n",
      "\n",
      "\n",
      "Epoch:95\n",
      "Validation Loss: 1.9323710203170776\n",
      "Train Loss:      1.9248848011464248\n",
      "\n",
      "\n",
      "Epoch:96\n",
      "Validation Loss: 1.9302068948745728\n",
      "Train Loss:      1.9244438027158195\n",
      "\n",
      "\n",
      "Epoch:97\n",
      "Validation Loss: 1.9346308708190918\n",
      "Train Loss:      1.924878876591906\n",
      "\n",
      "\n",
      "Epoch:98\n",
      "Validation Loss: 1.93303644657135\n",
      "Train Loss:      1.9249587403403388\n",
      "\n",
      "\n",
      "Epoch:99\n",
      "Validation Loss: 1.9310667514801025\n",
      "Train Loss:      1.924539985185788\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(decoder.parameters(), amsgrad=False, weight_decay=0.0,lr=0.005)\n",
    "\n",
    "trn_loss = nn.NLLLoss(weight=freqs.float())\n",
    "val_loss = nn.NLLLoss()\n",
    "\n",
    "for epoch in range(100):\n",
    "    print(\"Epoch:\" + str(epoch))\n",
    "    epoch_trn_loss_sum = 0\n",
    "    for batch in (range(batches)):\n",
    "        x_batch = x_trn[batch_size*batch:batch_size*(batch+1)]\n",
    "        y_batch = y_trn[batch_size*batch:batch_size*(batch+1)]\n",
    "        \n",
    "        out = decoder.forward(x_batch)\n",
    "        log_softmax = F.log_softmax(out, dim=1).unsqueeze(2)\n",
    "       # print(log_softmax.shape)\n",
    "       \n",
    "\n",
    "      \n",
    "        NLL_Loss = trn_loss(log_softmax,y_batch.long()) \n",
    "        optimizer.zero_grad()\n",
    "        NLL_Loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_trn_loss_sum += NLL_Loss.item()\n",
    "\n",
    "    out_val = decoder.forward(x_val)\n",
    "    log_val_softmax = F.log_softmax(out_val, dim=1).unsqueeze(2)\n",
    "    NLL_val_loss = trn_loss(log_val_softmax, y_val.long())\n",
    "    \n",
    "    epoch_trn_loss = epoch_trn_loss_sum/batches\n",
    "    print(\"Validation Loss: \"+str(NLL_val_loss.item()))\n",
    "    print(\"Train Loss:      \"+str(epoch_trn_loss))\n",
    "    print(\"\\n\")\n",
    "   # print(\"Val: \"+str(val_loss.item()))\n",
    "    metric_dict['losses_trn'].append(NLL_Loss.item())\n",
    "    metric_dict['losses_val'].append(epoch_trn_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd060f6cd50>]"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(metric_dict['losses_trn'][:100])\n",
    "plt.plot(metric_dict['losses_val'][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/timlongley/opt/anaconda3/envs/project/lib/python3.7/site-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-327-41a36cb10410>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mcovariance_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcov\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcovariance_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Sort descending and get sorted indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36meig\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/project/lib/python3.7/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36meig\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   1316\u001b[0m     \u001b[0m_assert_stacked_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m     \u001b[0m_assert_stacked_square\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1318\u001b[0;31m     \u001b[0m_assert_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1319\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_commonType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/project/lib/python3.7/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_assert_finite\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Array must not contain infs or NaNs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_empty_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Array must not contain infs or NaNs"
     ]
    }
   ],
   "source": [
    "embeddings = np.zeros(shape=(len(vocab),15))\n",
    "\n",
    "i=0\n",
    "for grapheme in vocab:\n",
    "    test = graph2vec(grapheme)\n",
    "    embedding = decoder.encode(test).detach().numpy()\n",
    "    embeddings[i,:]=embedding\n",
    "    i+=1\n",
    "\n",
    "\n",
    "\n",
    "embedding_means = embeddings.mean(axis=0)\n",
    "embedding_std = embeddings.std(axis=0)\n",
    "norm_embeddings = (embeddings-embedding_means)/embedding_std\n",
    "\n",
    "    \n",
    "covariance_matrix = np.cov(norm_embeddings.T)\n",
    "v,w = np.linalg.eig(covariance_matrix)\n",
    "\n",
    "idx = v.argsort()[::-1] # Sort descending and get sorted indices\n",
    "v = v[idx] # Use indices on eigv vector\n",
    "w = w[:,idx] # \n",
    "\n",
    "variance_explained = []\n",
    "for i in v:\n",
    "     variance_explained.append((i/sum(v))*100)\n",
    "        \n",
    "red_Vecs = w[0:2,:]\n",
    "\n",
    "low_d_embed = (embeddings @ red_Vecs.T)\n",
    "\n",
    "#plt.scatter(x=low_d_embed[:,0],y=low_d_embed[:,1])\n",
    "\n",
    "x=low_d_embed[:,0]\n",
    "y=low_d_embed[:,1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y,color='white')\n",
    "\n",
    "for i, txt in enumerate(vocab):\n",
    "    ax.annotate(txt, (x[i], y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n', 'd']\n",
      "Prediction: nd e\n",
      "True:       nd u\n"
     ]
    }
   ],
   "source": [
    "n=randint(0,x_trn.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "input = x_trn[n,:,:].unsqueeze(0)\n",
    "output = y_trn[n,:]\n",
    "\n",
    "out=decoder.forward(input)\n",
    "log_softmax = -F.log_softmax(out, dim=1)\n",
    "\n",
    "p=(log_softmax.argmin(dim=1).detach().numpy()[0])\n",
    "o=(output.int().numpy())\n",
    "\n",
    "numpy_input = input.int().detach().numpy()[0]\n",
    "inp_ind = (numpy_input.argmax(axis=1))\n",
    "inp_let = [vocab[i] for i in inp_ind]\n",
    "\n",
    "\n",
    "print(inp_let)\n",
    "print(\"Prediction: \" + ''.join(inp_let)+\" \"+str(vocab[p]))\n",
    "print(\"True:       \" + ''.join(inp_let)+\" \"+str(vocab[o[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$', '$', 'p', 'r', 'i', 'm', 'u', 'l', 'a']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-390-942018dea318>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgraph2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "n=randint(0,len(tokens))\n",
    "token=(tokens[n])[:-1]\n",
    "print(token)\n",
    "\n",
    "input = token[-3:-1]\n",
    "\n",
    "\n",
    "#input = x_trn[n,:,:].unsqueeze(0)\n",
    "output = token[-1:][0]\n",
    "\n",
    "input = torch.tensor([graph2vec(input[0]),graph2vec(input[1])]).unsqueeze(0).float()\n",
    "\n",
    "out=decoder.forward(input)\n",
    "log_softmax = -F.log_softmax(out, dim=1)\n",
    "\n",
    "p=(log_softmax.argmin(dim=1).detach().numpy()[0])\n",
    "o=(output)\n",
    "\n",
    "numpy_input = input.int().detach().numpy()[0]\n",
    "inp_ind = (numpy_input.argmax(axis=1))\n",
    "inp_let = [vocab[i] for i in inp_ind]\n",
    "print(inp_let)\n",
    "print(p)\n",
    "\n",
    "\n",
    "print(inp_let)\n",
    "print(\"Prediction: \" + ''.join(token[:-1])+\" \"+str(vocab[p]))\n",
    "print(\"True:       \" + ''.join(token[:-1])+\" \"+str(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secenion tie nuisies saticalarrin imply tasing pas sallers unens dister\n"
     ]
    }
   ],
   "source": [
    "n=randint(0,len(tokens))\n",
    "token=(tokens[n])[:-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    sentence = []\n",
    "    for i in range(10):\n",
    "        string = \"$$\"\n",
    "\n",
    "        while True:\n",
    "\n",
    "            input=string[-2:]\n",
    "            input = torch.stack((graph2vec(input[0]), graph2vec(input[1]))).unsqueeze(0)\n",
    "\n",
    "            out=decoder.forward(input)\n",
    "            probs = F.softmax(out, dim=1).detach().numpy()[0]\n",
    "\n",
    "            sample = np.random.multinomial(1, probs)\n",
    "\n",
    "            ids = np.argmax(sample)\n",
    "            grapheme = str(vocab[ids])\n",
    "            #p=(log_softmax.argmin(dim=1).detach().numpy()[0])\n",
    "\n",
    "            if grapheme==\"$\":\n",
    "                continue\n",
    "            string+=grapheme\n",
    "            if grapheme ==\"£\":\n",
    "                break\n",
    "\n",
    "        string = string.split('$')[2:][0]\n",
    "        string = string.split('£')[:-1][0]\n",
    "        sentence.append(string)\n",
    "\n",
    "    print(' '.join(sentence))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbour of a : c\n",
      "Nearest neighbour of b : p\n",
      "Nearest neighbour of c : f\n",
      "Nearest neighbour of d : z\n",
      "Nearest neighbour of e : s\n",
      "Nearest neighbour of f : x\n",
      "Nearest neighbour of g : q\n",
      "Nearest neighbour of h : r\n",
      "Nearest neighbour of i : k\n",
      "Nearest neighbour of j : w\n",
      "Nearest neighbour of k : i\n",
      "Nearest neighbour of l : d\n",
      "Nearest neighbour of m : x\n",
      "Nearest neighbour of n : r\n",
      "Nearest neighbour of o : a\n",
      "Nearest neighbour of p : b\n",
      "Nearest neighbour of q : s\n",
      "Nearest neighbour of r : n\n",
      "Nearest neighbour of s : t\n",
      "Nearest neighbour of t : s\n",
      "Nearest neighbour of u : b\n",
      "Nearest neighbour of v : $\n",
      "Nearest neighbour of w : $\n",
      "Nearest neighbour of x : f\n",
      "Nearest neighbour of y : t\n",
      "Nearest neighbour of z : £\n",
      "Nearest neighbour of $ : v\n",
      "Nearest neighbour of £ : z\n"
     ]
    }
   ],
   "source": [
    "embeddings = np.zeros(shape=(len(vocab),10))\n",
    "\n",
    "i=0\n",
    "for grapheme in vocab:\n",
    "    test = torch.from_numpy(graph2vec(grapheme)).float()\n",
    "    embedding = decoder.encode(test).detach().numpy()\n",
    "    embeddings[i,:]=embedding\n",
    "    i+=1\n",
    "    \n",
    "l=0\n",
    "for e in (embeddings):\n",
    "    dists=(np.square(embeddings-e).sum(axis=1))\n",
    "    nearest_idx=(np.argsort(dists)[1])\n",
    "    print(\"Nearest neighbour of \"+vocab[l]+\" : \"+vocab[nearest_idx])\n",
    "    l+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'1'.isnumeric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros(3,15,100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(a,'test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(x_data,'EnglishData.pt')\n",
    "torch.save(y_data,'EnglishLabels.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(freqs,'EnglishFreqs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
