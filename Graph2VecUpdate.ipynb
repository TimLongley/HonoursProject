{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grapheme Embedding Using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import tqdm\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'e', 'h', 'l', 'o'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"graphemes.txt\")\n",
    "processed_text = (re.sub(r'[^\\w\\s]','',text.read().lower()))\n",
    "\n",
    "vocab = list('abcdefghijklmnopqrstuvwxyz$£')\n",
    "\n",
    "tokens = [list(\"$$\"+token+\"£\") for token in processed_text.split('\\n') if set(token)&set(vocab)==set(token)]\n",
    "\n",
    "\n",
    "v = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9eb0c4dfe976>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopOc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mpair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pairFreqs={}\n",
    "replacements = {}\n",
    "\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    tempTokens = []\n",
    "    for token in tokens:\n",
    "        if i!=0:\n",
    "            token = list(''.join(token).replace(topOc,str(i-1)))\n",
    "        for g in range(2,len(token)-2):\n",
    "            pair = (token[g]+token[g+1])\n",
    "            if pair in pairFreqs.keys():\n",
    "                pairFreqs[pair] += 1\n",
    "            else:\n",
    "                pairFreqs[pair] = 1\n",
    "        tempTokens.append(token)\n",
    "        \n",
    "    tokens = tempTokens\n",
    "    topOcs = [(k, v) for k, v in sorted(pairFreqs.items(), key=lambda item: item[1],reverse=True)]\n",
    "    topOc = (topOcs[0][0])\n",
    "    \n",
    "    replacements[i] = topOc\n",
    "print(topOc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph2int(graph):\n",
    "    return vocab.index(graph)\n",
    "    \n",
    "def int2graph(index):\n",
    "    return vocab[index]\n",
    "\n",
    "def int2vec(integer):\n",
    "    vec=np.zeros(len(vocab))\n",
    "    vec[integer]=1\n",
    "    return vec\n",
    "\n",
    "def graph2vec(graph):\n",
    "    return (int2vec(graph2int(graph)))\n",
    "\n",
    "def vec2graph(vec):\n",
    "    return (int2graph(np.argmax(vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$', '$', 'e', 'x', 'c', 'i', 't', 'a', 't', 'i', 'o', 'n', 's', '£']\n"
     ]
    }
   ],
   "source": [
    "windowSize = 2\n",
    "tokens = [tokenSet for tokenSet in tokens if len(tokenSet)>=6]\n",
    "print(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 695/5000 [00:04<00:25, 169.23it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1f3afd6c44e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mwc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mgraph_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwc\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mdata_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph_i\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mx_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_sample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mlabel_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph2int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph_i\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-0057a7694e70>\u001b[0m in \u001b[0;36mgraph2vec\u001b[0;34m(graph)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgraph2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph2int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvec2graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-0057a7694e70>\u001b[0m in \u001b[0;36mint2vec\u001b[0;34m(integer)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mint2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minteger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mvec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mvec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minteger\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_data = torch.empty(size=(1,2,38))\n",
    "y_data = torch.empty(size=(1,1))\n",
    "\n",
    "freqs= torch.zeros(size=(1,38))\n",
    "\n",
    "i=0\n",
    "N=len(tokens)\n",
    "for word in tqdm.tqdm(tokens[:5000]):\n",
    "    wc = len(word)\n",
    "    for graph_i in range(wc-2): \n",
    "        data_sample = torch.tensor([graph2vec(word[graph_i]), graph2vec(word[graph_i+1])]).unsqueeze(0).float()\n",
    "        x_data = torch.cat([x_data,data_sample])\n",
    "        label_sample = torch.tensor([graph2int(word[graph_i+2])]).unsqueeze(0).float()\n",
    "        y_data = torch.cat([y_data,label_sample])\n",
    "        freqs += graph2vec(word[graph_i+2])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "x_data=torch.load('EnglishData.pt')\n",
    "y_data=torch.load('EnglishLabels.pt')\n",
    "freqs=torch.load('EnglishFreqs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([129814, 2, 28])\n",
      "torch.Size([129814, 1])\n",
      "torch.Size([1, 28])\n"
     ]
    }
   ],
   "source": [
    "print(x_data.shape)\n",
    "print(y_data.shape)\n",
    "print(freqs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,grapheme_shape,hidden_units,embedding_units):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.word_shape = grapheme_shape\n",
    "        self.hidden_units = hidden_units\n",
    "        self.embedding_units = embedding_units\n",
    "        \n",
    "        self.weights_1 = nn.Parameter(torch.empty(size=(hidden_units, grapheme_shape), requires_grad=True))\n",
    "        nn.init.normal_(self.weights_1)\n",
    "        \n",
    "        self.weights_2 = nn.Parameter(torch.empty(size=(embedding_units, hidden_units), requires_grad=True))\n",
    "        nn.init.normal_(self.weights_2)\n",
    "        \n",
    "        self.bias1 = nn.Parameter(torch.zeros(hidden_units), requires_grad=True)\n",
    "        self.bias2 = nn.Parameter(torch.zeros(embedding_units), requires_grad=True)\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        out = F.linear(inputs, self.weights_1, self.bias1)\n",
    "        out = nn.ReLU().forward(out)\n",
    "        out = F.linear(out, self.weights_2,self.bias2)\n",
    "        out = nn.ReLU().forward(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,embedding_units,layer_1_n,grapheme_shape,encoder):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.e = encoder\n",
    "        self.embedding_units=embedding_units\n",
    "        self.layer_1_n = layer_1_n\n",
    "        self.layer_2_n = layer_1_n\n",
    "\n",
    "        self.weights_layer1 = nn.Parameter(torch.empty(size=(self.layer_1_n, self.embedding_units*2), requires_grad=True))\n",
    "        nn.init.normal_(self.weights_layer1)\n",
    "        self.bias_layer1 = nn.Parameter(torch.zeros(layer_1_n), requires_grad=True)\n",
    "\n",
    "        self.weights_layer2 = nn.Parameter(torch.empty(size=(self.layer_2_n, self.layer_1_n), requires_grad=True))\n",
    "        nn.init.normal_(self.weights_layer2)\n",
    "        self.bias_layer2 = nn.Parameter(torch.zeros(self.layer_2_n), requires_grad=True)\n",
    "        \n",
    "        self.weights_output = nn.Parameter(torch.empty(size=(grapheme_shape, self.layer_1_n), requires_grad=True))\n",
    "        nn.init.normal_(self.weights_output)\n",
    "        self.bias = nn.Parameter(torch.zeros(grapheme_shape), requires_grad=True)\n",
    "        \n",
    "      \n",
    "    def forward(self,inputs):\n",
    "        embedding1 = self.e.forward(inputs[:,0])\n",
    "        embedding2 = self.e.forward(inputs[:,1])\n",
    "\n",
    "        out = torch.cat((embedding1,embedding2),dim=1)\n",
    "\n",
    "        out = nn.ReLU().forward(F.linear(out, self.weights_layer1, self.bias_layer1))\n",
    "\n",
    "        out = nn.ReLU().forward(F.linear(out, self.weights_layer2, self.bias_layer2))\n",
    "        \n",
    "        out = nn.ReLU().forward(F.linear(out, self.weights_output, self.bias))\n",
    "   \n",
    "        return out\n",
    "    \n",
    "    def encode(self,inputs):\n",
    "        return self.e.forward(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.80\n",
    "batch_size = 256\n",
    "\n",
    "trn_n = int(x_data.shape[0] * 0.8)\n",
    "batches = (int(trn_n/batch_size))\n",
    "\n",
    "x_trn = x_data[:trn_n,:,:].clone()\n",
    "y_trn = y_data[:trn_n,:].clone()\n",
    "\n",
    "x_val = x_data[trn_n:,:,:].clone()\n",
    "y_val = y_data[trn_n:,:].clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0\n",
      "Validation Loss: 15.966716766357422\n",
      "Train Loss:      59.42518197518808\n",
      "\n",
      "\n",
      "Epoch:1\n",
      "Validation Loss: 4.704922199249268\n",
      "Train Loss:      8.952728973200292\n",
      "\n",
      "\n",
      "Epoch:2\n",
      "Validation Loss: 3.3570566177368164\n",
      "Train Loss:      3.874205462726546\n",
      "\n",
      "\n",
      "Epoch:3\n",
      "Validation Loss: 2.987774133682251\n",
      "Train Loss:      3.14289675347599\n",
      "\n",
      "\n",
      "Epoch:4\n",
      "Validation Loss: 2.863957166671753\n",
      "Train Loss:      2.909143301292702\n",
      "\n",
      "\n",
      "Epoch:5\n",
      "Validation Loss: 2.812741279602051\n",
      "Train Loss:      2.824707489837835\n",
      "\n",
      "\n",
      "Epoch:6\n",
      "Validation Loss: 2.789860248565674\n",
      "Train Loss:      2.7884931923430645\n",
      "\n",
      "\n",
      "Epoch:7\n",
      "Validation Loss: 2.7795228958129883\n",
      "Train Loss:      2.7723638187220065\n",
      "\n",
      "\n",
      "Epoch:8\n",
      "Validation Loss: 2.7736759185791016\n",
      "Train Loss:      2.7626392034836758\n",
      "\n",
      "\n",
      "Epoch:9\n",
      "Validation Loss: 2.7607083320617676\n",
      "Train Loss:      2.7508598839795146\n",
      "\n",
      "\n",
      "Epoch:10\n",
      "Validation Loss: 2.7523155212402344\n",
      "Train Loss:      2.7392492123592045\n",
      "\n",
      "\n",
      "Epoch:11\n",
      "Validation Loss: 2.74225115776062\n",
      "Train Loss:      2.728607428515399\n",
      "\n",
      "\n",
      "Epoch:12\n",
      "Validation Loss: 2.73478627204895\n",
      "Train Loss:      2.7211729903279998\n",
      "\n",
      "\n",
      "Epoch:13\n",
      "Validation Loss: 2.7242839336395264\n",
      "Train Loss:      2.7148970074123806\n",
      "\n",
      "\n",
      "Epoch:14\n",
      "Validation Loss: 2.7181296348571777\n",
      "Train Loss:      2.708475660983427\n",
      "\n",
      "\n",
      "Epoch:15\n",
      "Validation Loss: 2.7058260440826416\n",
      "Train Loss:      2.6947802555413896\n",
      "\n",
      "\n",
      "Epoch:16\n",
      "Validation Loss: 2.701829671859741\n",
      "Train Loss:      2.68789110419191\n",
      "\n",
      "\n",
      "Epoch:17\n",
      "Validation Loss: 2.6959896087646484\n",
      "Train Loss:      2.6834030692960007\n",
      "\n",
      "\n",
      "Epoch:18\n",
      "Validation Loss: 2.692021131515503\n",
      "Train Loss:      2.6791295987588386\n",
      "\n",
      "\n",
      "Epoch:19\n",
      "Validation Loss: 2.689333915710449\n",
      "Train Loss:      2.676324203867971\n",
      "\n",
      "\n",
      "Epoch:20\n",
      "Validation Loss: 2.687180757522583\n",
      "Train Loss:      2.673395809715177\n",
      "\n",
      "\n",
      "Epoch:21\n",
      "Validation Loss: 2.6795547008514404\n",
      "Train Loss:      2.668924707247887\n",
      "\n",
      "\n",
      "Epoch:22\n",
      "Validation Loss: 2.677175283432007\n",
      "Train Loss:      2.665215324472498\n",
      "\n",
      "\n",
      "Epoch:23\n",
      "Validation Loss: 2.6711316108703613\n",
      "Train Loss:      2.66106282811106\n",
      "\n",
      "\n",
      "Epoch:24\n",
      "Validation Loss: 2.668084144592285\n",
      "Train Loss:      2.6542474417038906\n",
      "\n",
      "\n",
      "Epoch:25\n",
      "Validation Loss: 2.664379835128784\n",
      "Train Loss:      2.650411093676532\n",
      "\n",
      "\n",
      "Epoch:26\n",
      "Validation Loss: 2.663057327270508\n",
      "Train Loss:      2.6479574586138312\n",
      "\n",
      "\n",
      "Epoch:27\n",
      "Validation Loss: 2.6605262756347656\n",
      "Train Loss:      2.6456228244451827\n",
      "\n",
      "\n",
      "Epoch:28\n",
      "Validation Loss: 2.658843755722046\n",
      "Train Loss:      2.6418245845370825\n",
      "\n",
      "\n",
      "Epoch:29\n",
      "Validation Loss: 2.658306837081909\n",
      "Train Loss:      2.6401937431759306\n",
      "\n",
      "\n",
      "Epoch:30\n",
      "Validation Loss: 2.655756950378418\n",
      "Train Loss:      2.638886554741565\n",
      "\n",
      "\n",
      "Epoch:31\n",
      "Validation Loss: 2.6548523902893066\n",
      "Train Loss:      2.637429818989318\n",
      "\n",
      "\n",
      "Epoch:32\n",
      "Validation Loss: 2.654096841812134\n",
      "Train Loss:      2.6364012906580796\n",
      "\n",
      "\n",
      "Epoch:33\n",
      "Validation Loss: 2.651594877243042\n",
      "Train Loss:      2.63558731785527\n",
      "\n",
      "\n",
      "Epoch:34\n",
      "Validation Loss: 2.6496188640594482\n",
      "Train Loss:      2.6339914569148313\n",
      "\n",
      "\n",
      "Epoch:35\n",
      "Validation Loss: 2.6488382816314697\n",
      "Train Loss:      2.6326932765819406\n",
      "\n",
      "\n",
      "Epoch:36\n",
      "Validation Loss: 2.643765449523926\n",
      "Train Loss:      2.6304261007426697\n",
      "\n",
      "\n",
      "Epoch:37\n",
      "Validation Loss: 2.639145612716675\n",
      "Train Loss:      2.626510782595034\n",
      "\n",
      "\n",
      "Epoch:38\n",
      "Validation Loss: 2.6363070011138916\n",
      "Train Loss:      2.623561140342995\n",
      "\n",
      "\n",
      "Epoch:39\n",
      "Validation Loss: 2.6332919597625732\n",
      "Train Loss:      2.6205540698251606\n",
      "\n",
      "\n",
      "Epoch:40\n",
      "Validation Loss: 2.6309893131256104\n",
      "Train Loss:      2.618410464863718\n",
      "\n",
      "\n",
      "Epoch:41\n",
      "Validation Loss: 2.628967523574829\n",
      "Train Loss:      2.616543268274378\n",
      "\n",
      "\n",
      "Epoch:42\n",
      "Validation Loss: 2.6265268325805664\n",
      "Train Loss:      2.6144005863754836\n",
      "\n",
      "\n",
      "Epoch:43\n",
      "Validation Loss: 2.6247661113739014\n",
      "Train Loss:      2.6126651763916016\n",
      "\n",
      "\n",
      "Epoch:44\n",
      "Validation Loss: 2.6208107471466064\n",
      "Train Loss:      2.610454315609402\n",
      "\n",
      "\n",
      "Epoch:45\n",
      "Validation Loss: 2.612287759780884\n",
      "Train Loss:      2.6036453676812443\n",
      "\n",
      "\n",
      "Epoch:46\n",
      "Validation Loss: 2.6075215339660645\n",
      "Train Loss:      2.59702239330904\n",
      "\n",
      "\n",
      "Epoch:47\n",
      "Validation Loss: 2.6061320304870605\n",
      "Train Loss:      2.592424088937265\n",
      "\n",
      "\n",
      "Epoch:48\n",
      "Validation Loss: 2.5990567207336426\n",
      "Train Loss:      2.5880538416497503\n",
      "\n",
      "\n",
      "Epoch:49\n",
      "Validation Loss: 2.5884783267974854\n",
      "Train Loss:      2.5828691523752094\n",
      "\n",
      "\n",
      "Epoch:50\n",
      "Validation Loss: 2.5782322883605957\n",
      "Train Loss:      2.5707923830291373\n",
      "\n",
      "\n",
      "Epoch:51\n",
      "Validation Loss: 2.568068742752075\n",
      "Train Loss:      2.5618537143424707\n",
      "\n",
      "\n",
      "Epoch:52\n",
      "Validation Loss: 2.5630125999450684\n",
      "Train Loss:      2.5544268243106796\n",
      "\n",
      "\n",
      "Epoch:53\n",
      "Validation Loss: 2.555530309677124\n",
      "Train Loss:      2.546645501807884\n",
      "\n",
      "\n",
      "Epoch:54\n",
      "Validation Loss: 2.5499720573425293\n",
      "Train Loss:      2.540054780465585\n",
      "\n",
      "\n",
      "Epoch:55\n",
      "Validation Loss: 2.5444672107696533\n",
      "Train Loss:      2.530685924011984\n",
      "\n",
      "\n",
      "Epoch:56\n",
      "Validation Loss: 2.537768840789795\n",
      "Train Loss:      2.525149502577605\n",
      "\n",
      "\n",
      "Epoch:57\n",
      "Validation Loss: 2.530423641204834\n",
      "Train Loss:      2.520583917476513\n",
      "\n",
      "\n",
      "Epoch:58\n",
      "Validation Loss: 2.5165324211120605\n",
      "Train Loss:      2.5108291696619105\n",
      "\n",
      "\n",
      "Epoch:59\n",
      "Validation Loss: 2.502995252609253\n",
      "Train Loss:      2.498141621365959\n",
      "\n",
      "\n",
      "Epoch:60\n",
      "Validation Loss: 2.491539716720581\n",
      "Train Loss:      2.4857977867126464\n",
      "\n",
      "\n",
      "Epoch:61\n",
      "Validation Loss: 2.4823224544525146\n",
      "Train Loss:      2.475950042701062\n",
      "\n",
      "\n",
      "Epoch:62\n",
      "Validation Loss: 2.4767560958862305\n",
      "Train Loss:      2.467721715974219\n",
      "\n",
      "\n",
      "Epoch:63\n",
      "Validation Loss: 2.469756603240967\n",
      "Train Loss:      2.4611820650689396\n",
      "\n",
      "\n",
      "Epoch:64\n",
      "Validation Loss: 2.4650964736938477\n",
      "Train Loss:      2.4568409001385723\n",
      "\n",
      "\n",
      "Epoch:65\n",
      "Validation Loss: 2.461350440979004\n",
      "Train Loss:      2.451945881784698\n",
      "\n",
      "\n",
      "Epoch:66\n",
      "Validation Loss: 2.4513771533966064\n",
      "Train Loss:      2.445619312333472\n",
      "\n",
      "\n",
      "Epoch:67\n",
      "Validation Loss: 2.4391353130340576\n",
      "Train Loss:      2.4352979359803375\n",
      "\n",
      "\n",
      "Epoch:68\n",
      "Validation Loss: 2.4354054927825928\n",
      "Train Loss:      2.4280750592549643\n",
      "\n",
      "\n",
      "Epoch:69\n",
      "Validation Loss: 2.429090976715088\n",
      "Train Loss:      2.422910354755543\n",
      "\n",
      "\n",
      "Epoch:70\n",
      "Validation Loss: 2.4215481281280518\n",
      "Train Loss:      2.414977428059519\n",
      "\n",
      "\n",
      "Epoch:71\n",
      "Validation Loss: 2.4132864475250244\n",
      "Train Loss:      2.4069968023417907\n",
      "\n",
      "\n",
      "Epoch:72\n",
      "Validation Loss: 2.40592622756958\n",
      "Train Loss:      2.3994978062900496\n",
      "\n",
      "\n",
      "Epoch:73\n",
      "Validation Loss: 2.400143623352051\n",
      "Train Loss:      2.3937645959265437\n",
      "\n",
      "\n",
      "Epoch:74\n",
      "Validation Loss: 2.3951587677001953\n",
      "Train Loss:      2.3893849490601338\n",
      "\n",
      "\n",
      "Epoch:75\n",
      "Validation Loss: 2.3900351524353027\n",
      "Train Loss:      2.384515740547651\n",
      "\n",
      "\n",
      "Epoch:76\n",
      "Validation Loss: 2.3837890625\n",
      "Train Loss:      2.3798606272096987\n",
      "\n",
      "\n",
      "Epoch:77\n",
      "Validation Loss: 2.3751060962677\n",
      "Train Loss:      2.371185827255249\n",
      "\n",
      "\n",
      "Epoch:78\n",
      "Validation Loss: 2.3571205139160156\n",
      "Train Loss:      2.3570276060222106\n",
      "\n",
      "\n",
      "Epoch:79\n",
      "Validation Loss: 2.3428351879119873\n",
      "Train Loss:      2.3422669393044933\n",
      "\n",
      "\n",
      "Epoch:80\n",
      "Validation Loss: 2.328718423843384\n",
      "Train Loss:      2.3270510567559137\n",
      "\n",
      "\n",
      "Epoch:81\n",
      "Validation Loss: 2.321422815322876\n",
      "Train Loss:      2.3167331254040753\n",
      "\n",
      "\n",
      "Epoch:82\n",
      "Validation Loss: 2.3136723041534424\n",
      "Train Loss:      2.3105028535112924\n",
      "\n",
      "\n",
      "Epoch:83\n",
      "Validation Loss: 2.3107151985168457\n",
      "Train Loss:      2.305907713336709\n",
      "\n",
      "\n",
      "Epoch:84\n",
      "Validation Loss: 2.305421829223633\n",
      "Train Loss:      2.3017398510450198\n",
      "\n",
      "\n",
      "Epoch:85\n",
      "Validation Loss: 2.297107219696045\n",
      "Train Loss:      2.296504262641624\n",
      "\n",
      "\n",
      "Epoch:86\n",
      "Validation Loss: 2.2869064807891846\n",
      "Train Loss:      2.2880323633735564\n",
      "\n",
      "\n",
      "Epoch:87\n",
      "Validation Loss: 2.2778499126434326\n",
      "Train Loss:      2.277623779391065\n",
      "\n",
      "\n",
      "Epoch:88\n",
      "Validation Loss: 2.2722113132476807\n",
      "Train Loss:      2.271306570076648\n",
      "\n",
      "\n",
      "Epoch:89\n",
      "Validation Loss: 2.2657246589660645\n",
      "Train Loss:      2.266949489970266\n",
      "\n",
      "\n",
      "Epoch:90\n",
      "Validation Loss: 2.2626283168792725\n",
      "Train Loss:      2.260706197479625\n",
      "\n",
      "\n",
      "Epoch:91\n",
      "Validation Loss: 2.2570133209228516\n",
      "Train Loss:      2.25578982859482\n",
      "\n",
      "\n",
      "Epoch:92\n",
      "Validation Loss: 2.248189926147461\n",
      "Train Loss:      2.249450045456121\n",
      "\n",
      "\n",
      "Epoch:93\n",
      "Validation Loss: 2.2378456592559814\n",
      "Train Loss:      2.239553800629981\n",
      "\n",
      "\n",
      "Epoch:94\n",
      "Validation Loss: 2.2317049503326416\n",
      "Train Loss:      2.232803494841964\n",
      "\n",
      "\n",
      "Epoch:95\n",
      "Validation Loss: 2.2265563011169434\n",
      "Train Loss:      2.2270302489951805\n",
      "\n",
      "\n",
      "Epoch:96\n",
      "Validation Loss: 2.2232425212860107\n",
      "Train Loss:      2.2225757946202784\n",
      "\n",
      "\n",
      "Epoch:97\n",
      "Validation Loss: 2.2203526496887207\n",
      "Train Loss:      2.219337479273478\n",
      "\n",
      "\n",
      "Epoch:98\n",
      "Validation Loss: 2.21891713142395\n",
      "Train Loss:      2.217100515483338\n",
      "\n",
      "\n",
      "Epoch:99\n",
      "Validation Loss: 2.2168338298797607\n",
      "Train Loss:      2.2148596816592745\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metric_dict = {'losses_trn': [],'losses_val':[]} \n",
    "embed_units = 15\n",
    "encoder =  Encoder(grapheme_shape=28,hidden_units=15,embedding_units=embed_units)\n",
    "decoder = Decoder(embedding_units=embed_units,layer_1_n=32,grapheme_shape=28,encoder=encoder)\n",
    "optimizer = optim.Adam(decoder.parameters(), amsgrad=False, weight_decay=0.0,lr=0.001)\n",
    "\n",
    "trn_loss = nn.NLLLoss(weight=freqs.float())\n",
    "val_loss = nn.NLLLoss()\n",
    "\n",
    "for epoch in range(100):\n",
    "    print(\"Epoch:\" + str(epoch))\n",
    "    epoch_trn_loss_sum = 0\n",
    "    for batch in (range(batches)):\n",
    "        x_batch = x_trn[batch_size*batch:batch_size*(batch+1)]\n",
    "        y_batch = y_trn[batch_size*batch:batch_size*(batch+1)]\n",
    "        \n",
    "        out = decoder.forward(x_batch)\n",
    "        log_softmax = F.log_softmax(out, dim=1).unsqueeze(2)\n",
    "        \n",
    "       \n",
    "        \n",
    "        NLL_Loss = trn_loss(log_softmax,y_batch.long()) \n",
    "        optimizer.zero_grad()\n",
    "        NLL_Loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_trn_loss_sum += NLL_Loss.item()\n",
    "        \n",
    "    out_val = decoder.forward(x_val)\n",
    "    log_val_softmax = F.log_softmax(out_val, dim=1).unsqueeze(2)\n",
    "    NLL_val_loss = trn_loss(log_val_softmax, y_val.long())\n",
    "    \n",
    "    epoch_trn_loss = epoch_trn_loss_sum/batches\n",
    "    print(\"Validation Loss: \"+str(NLL_val_loss.item()))\n",
    "    print(\"Train Loss:      \"+str(epoch_trn_loss))\n",
    "    print(\"\\n\")\n",
    "   # print(\"Val: \"+str(val_loss.item()))\n",
    "    metric_dict['losses_trn'].append(NLL_Loss.item())\n",
    "    metric_dict['losses_val'].append(epoch_trn_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7faab190d750>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(metric_dict['losses_trn'][:100])\n",
    "plt.plot(metric_dict['losses_val'][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = np.zeros(shape=(len(vocab),15))\n",
    "\n",
    "i=0\n",
    "for grapheme in vocab:\n",
    "    test = torch.from_numpy(graph2vec(grapheme)).float()\n",
    "    embedding = decoder.encode(test).detach().numpy()\n",
    "    embeddings[i,:]=embedding\n",
    "    i+=1\n",
    "\n",
    "\n",
    "\n",
    "embedding_means = embeddings.mean(axis=0)\n",
    "embedding_std = embeddings.std(axis=0)\n",
    "norm_embeddings = (embeddings-embedding_means)/embedding_std\n",
    "\n",
    "    \n",
    "covariance_matrix = np.cov(norm_embeddings.T)\n",
    "v,w = np.linalg.eig(covariance_matrix)\n",
    "\n",
    "idx = v.argsort()[::-1] # Sort descending and get sorted indices\n",
    "v = v[idx] # Use indices on eigv vector\n",
    "w = w[:,idx] # \n",
    "\n",
    "variance_explained = []\n",
    "for i in v:\n",
    "     variance_explained.append((i/sum(v))*100)\n",
    "        \n",
    "red_Vecs = w[0:2,:]\n",
    "\n",
    "low_d_embed = (embeddings @ red_Vecs.T)\n",
    "\n",
    "#plt.scatter(x=low_d_embed[:,0],y=low_d_embed[:,1])\n",
    "\n",
    "x=low_d_embed[:,0]\n",
    "y=low_d_embed[:,1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y,color='white')\n",
    "\n",
    "for i, txt in enumerate(vocab):\n",
    "    ax.annotate(txt, (x[i], y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p', 'u']\n",
      "Prediction: pu r\n",
      "True:       pu l\n"
     ]
    }
   ],
   "source": [
    "n=randint(0,x_trn.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "input = x_trn[n,:,:].unsqueeze(0)\n",
    "output = y_trn[n,:]\n",
    "\n",
    "out=decoder.forward(input)\n",
    "log_softmax = -F.log_softmax(out, dim=1)\n",
    "\n",
    "p=(log_softmax.argmin(dim=1).detach().numpy()[0])\n",
    "o=(output.int().numpy())\n",
    "\n",
    "numpy_input = input.int().detach().numpy()[0]\n",
    "inp_ind = (numpy_input.argmax(axis=1))\n",
    "inp_let = [vocab[i] for i in inp_ind]\n",
    "\n",
    "\n",
    "print(inp_let)\n",
    "print(\"Prediction: \" + ''.join(inp_let)+\" \"+str(vocab[p]))\n",
    "print(\"True:       \" + ''.join(inp_let)+\" \"+str(vocab[o[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$', '$', 'm', 'i', 's', 'c', 'e', 'l', 'l', 'a', 'n', 'e', 'o', 'u', 's']\n",
      "['o', 'u']\n",
      "18\n",
      "['o', 'u']\n",
      "Prediction: $$miscellaneou s\n",
      "True:       $$miscellaneou s\n",
      "$$\n"
     ]
    }
   ],
   "source": [
    "n=randint(0,len(tokens))\n",
    "token=(tokens[n])[:-1]\n",
    "print(token)\n",
    "\n",
    "input = token[-3:-1]\n",
    "\n",
    "\n",
    "#input = x_trn[n,:,:].unsqueeze(0)\n",
    "output = token[-1:][0]\n",
    "\n",
    "input = torch.tensor([graph2vec(input[0]),graph2vec(input[1])]).unsqueeze(0).float()\n",
    "\n",
    "out=decoder.forward(input)\n",
    "log_softmax = -F.log_softmax(out, dim=1)\n",
    "\n",
    "p=(log_softmax.argmin(dim=1).detach().numpy()[0])\n",
    "o=(output)\n",
    "\n",
    "numpy_input = input.int().detach().numpy()[0]\n",
    "inp_ind = (numpy_input.argmax(axis=1))\n",
    "inp_let = [vocab[i] for i in inp_ind]\n",
    "print(inp_let)\n",
    "print(p)\n",
    "\n",
    "\n",
    "print(inp_let)\n",
    "print(\"Prediction: \" + ''.join(token[:-1])+\" \"+str(vocab[p]))\n",
    "print(\"True:       \" + ''.join(token[:-1])+\" \"+str(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sharxalilpens ousenidnes sorzeshalerle wajxors ipelle sidles ree jadinoiccy sagenwers es\n"
     ]
    }
   ],
   "source": [
    "n=randint(0,len(tokens))\n",
    "token=(tokens[n])[:-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    sentence = []\n",
    "    for i in range(10):\n",
    "        string = \"$$\"\n",
    "\n",
    "        while True:\n",
    "\n",
    "            input=string[-2:]\n",
    "            input = torch.tensor([graph2vec(input[0]),graph2vec(input[1])]).unsqueeze(0).float()\n",
    "\n",
    "            out=decoder.forward(input)\n",
    "            probs = F.softmax(out, dim=1).detach().numpy()[0]\n",
    "\n",
    "            sample = np.random.multinomial(1, probs)\n",
    "\n",
    "            ids = np.argmax(sample)\n",
    "\n",
    "            #p=(log_softmax.argmin(dim=1).detach().numpy()[0])\n",
    "\n",
    "            grapheme = str(vocab[ids])\n",
    "            if grapheme.isnumeric():\n",
    "                grapheme = replacements[int(grapheme)]\n",
    "\n",
    "            if grapheme==\"$\":\n",
    "                continue\n",
    "            string+=grapheme\n",
    "            if grapheme ==\"£\":\n",
    "                break\n",
    "\n",
    "        string = string.split('$')[2:][0]\n",
    "        string = string.split('£')[:-1][0]\n",
    "        sentence.append(string)\n",
    "\n",
    "    print(' '.join(sentence))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbour of a : c\n",
      "Nearest neighbour of b : p\n",
      "Nearest neighbour of c : f\n",
      "Nearest neighbour of d : z\n",
      "Nearest neighbour of e : s\n",
      "Nearest neighbour of f : x\n",
      "Nearest neighbour of g : q\n",
      "Nearest neighbour of h : r\n",
      "Nearest neighbour of i : k\n",
      "Nearest neighbour of j : w\n",
      "Nearest neighbour of k : i\n",
      "Nearest neighbour of l : d\n",
      "Nearest neighbour of m : x\n",
      "Nearest neighbour of n : r\n",
      "Nearest neighbour of o : a\n",
      "Nearest neighbour of p : b\n",
      "Nearest neighbour of q : s\n",
      "Nearest neighbour of r : n\n",
      "Nearest neighbour of s : t\n",
      "Nearest neighbour of t : s\n",
      "Nearest neighbour of u : b\n",
      "Nearest neighbour of v : $\n",
      "Nearest neighbour of w : $\n",
      "Nearest neighbour of x : f\n",
      "Nearest neighbour of y : t\n",
      "Nearest neighbour of z : £\n",
      "Nearest neighbour of $ : v\n",
      "Nearest neighbour of £ : z\n"
     ]
    }
   ],
   "source": [
    "embeddings = np.zeros(shape=(len(vocab),10))\n",
    "\n",
    "i=0\n",
    "for grapheme in vocab:\n",
    "    test = torch.from_numpy(graph2vec(grapheme)).float()\n",
    "    embedding = decoder.encode(test).detach().numpy()\n",
    "    embeddings[i,:]=embedding\n",
    "    i+=1\n",
    "    \n",
    "l=0\n",
    "for e in (embeddings):\n",
    "    dists=(np.square(embeddings-e).sum(axis=1))\n",
    "    nearest_idx=(np.argsort(dists)[1])\n",
    "    print(\"Nearest neighbour of \"+vocab[l]+\" : \"+vocab[nearest_idx])\n",
    "    l+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'1'.isnumeric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
