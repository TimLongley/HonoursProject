{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting project research - 24/09/2020\n",
    "\n",
    "#### Beginning Research for Honours Project by implementing Word2Vec myself.\n",
    "\n",
    "The motivation behind this research is based on the assumption that graphemes and phonemes adhere to the distributional hypothesis in the same way words do. That is, the meaning of graphemes (and phonemes) lie within the words that surround them (context). I am hoping that if I can build a neural network model using Tensorflow for to implement Word2Vec, I can also design a similar model to create grapheme embeddings, and then hopefully phoneme embeddings.\n",
    "\n",
    "For this research I will use the bible.txt from project guntenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/timlongley/opt/anaconda3/envs/project/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/timlongley/opt/anaconda3/envs/project/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open Text File\n",
    "\n",
    "Open text file and add each tokenized sentence to data array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(\"data/bbc_samp.adj\",'r') as file:\n",
    "    file = re.sub(r'[^a-zA-Z\\s]','',file.read())\n",
    "    for line in file.split('\\n'):\n",
    "        tokens = word_tokenize(line.lower())\n",
    "        if len(tokens) > 5:\n",
    "            data.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a Vocabulary\n",
    "\n",
    "Go through the data and add each unique word to a vobabulary list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set([word for sent in data for word in sent]))\n",
    "vocab.sort()\n",
    "v = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create One-Hot vectors for Training Data\n",
    "For each word in the vocabulary create a one hot vector and map to corresponding word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2int(word):\n",
    "    return vocab.index(word)\n",
    "    \n",
    "def int2word(index):\n",
    "    return vocab[index]\n",
    "\n",
    "def int2vec(integer):\n",
    "    vec=np.zeros(len(vocab))\n",
    "    vec[integer]=1\n",
    "    return vec\n",
    "\n",
    "def word2vec(word):\n",
    "    return (int2vec(word2int(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training data\n",
    "\n",
    "Iterate through each line and for each word in the line, place a window around the word.\n",
    "Create the one hot vectors for words n to the left of the word and n to the right of the word.\n",
    "place these in a row vector and add to training input set. \n",
    "Then convert the centre word to a one hot and add it to the label data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size=2\n",
    "# use window_size words before and after the context word\n",
    "c=0\n",
    "for line in data:\n",
    "   c +=len(line)-window_size*2 \n",
    "\n",
    "c=c*(window_size*2)\n",
    "\n",
    "x_trn = np.empty(shape=(c,len(vocab)))\n",
    "y_trn = np.empty(shape=(c,len(vocab)))\n",
    "\n",
    "data_N = len(data)\n",
    "\n",
    "\n",
    "i = 0\n",
    "for line in data:\n",
    "    wc = len(line)\n",
    "    for term_i in range(2,wc-2):\n",
    "        x_trn[i:i+window_size*2,:]=word2vec(line[term_i]) \n",
    "        y_trn[i,:] = word2vec(line[term_i-2])\n",
    "        y_trn[i+1,:] = word2vec(line[term_i-1])\n",
    "        y_trn[i+2,:] = word2vec(line[term_i+1])\n",
    "        y_trn[i+3,:] = word2vec(line[term_i+2])\n",
    "        i+=window_size*2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old\n",
      "['the', 'first', 'book', 'of', 'moses', 'called', 'genesis']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 13030)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(int2word(int(np.multiply(y_trn[0,13030:13030*2],np.arange(13030)).sum())))\n",
    "print(data[1])\n",
    "x_trn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=(None, v))\n",
    "y = tf.placeholder(tf.float32, shape=(None, v))\n",
    "\n",
    "embedding_dim = 15\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([v,embedding_dim]))\n",
    "b1 = tf.Variable(tf.random_normal([embedding_dim]))\n",
    "\n",
    "hidden_layer = tf.add(tf.matmul(x,W1),b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([embedding_dim,v]))\n",
    "b2 = tf.Variable(tf.random_normal([v]))\n",
    "\n",
    "prediction = tf.nn.softmax(tf.add( tf.matmul(hidden_layer, W2), b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-38b07aa669ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcross_entropy_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(prediction), reduction_indices=[1]))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy_loss)\n",
    "n_iters = 10000\n",
    "# train for n_iter iterations\n",
    "for _ in range(n_iters):\n",
    "    sess.run(train_step, feed_dict={x: x_trn, y: y_trn})\n",
    "    print('loss is : ', sess.run(cross_entropy_loss, feed_dict={x: x_trn, y: y_trn}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "Maybe get a new dataset with longer sentences.\n",
    "Create training data - one word as input (x: a D dimension vector) and multiple words as output (y: stacked on top CxD dimension vector where C is training window).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [1., 2.],\n",
       "       [1., 2.],\n",
       "       [1., 2.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.empty(shape=(4,2))\n",
    "b= np.array([1,2])\n",
    "a[0:4]=b\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
